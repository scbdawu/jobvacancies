{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping date: 2020-02-07    [downloaded_number_of_ads:39053]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-11    [downloaded_number_of_ads:377412]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-12    [downloaded_number_of_ads:380087]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-13    [downloaded_number_of_ads:382552]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-14    [downloaded_number_of_ads:385766]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-15    [downloaded_number_of_ads:386129]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-16    [downloaded_number_of_ads:386901]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-17    [downloaded_number_of_ads:389594]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-18    [downloaded_number_of_ads:392733]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-19    [downloaded_number_of_ads:395434]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-20    [downloaded_number_of_ads:397847]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-21    [downloaded_number_of_ads:400860]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-22    [downloaded_number_of_ads:401192]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-23    [downloaded_number_of_ads:401746]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-24    [downloaded_number_of_ads:404169]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-25    [downloaded_number_of_ads:406989]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-26    [downloaded_number_of_ads:409427]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-27    [downloaded_number_of_ads:411663]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-28    [downloaded_number_of_ads:414521]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-29    [downloaded_number_of_ads:414564]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-03-01    [downloaded_number_of_ads:413486]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-03-02    [downloaded_number_of_ads:416343]\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Import libraries and processing the downloaded files to make them ready for DB\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import time\n",
    "from multiprocessing import Pool \n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "path = \"/home/inlab4/Documents/Dan_datasets/AF/\"\n",
    "file_dump=[]\n",
    "#print(type(file_dump))\n",
    "#the file is saved in json format\n",
    "for f in glob.iglob(path+\"*.json\"):\n",
    "    with open(f, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        date = datetime.date.fromisoformat(f.split('/')[-1].replace(\".json\", \"\")[0:10])\n",
    "        number_ads = len(data)\n",
    "        summary = {\"date\": date, \"number_ads\": number_ads, \"file\": f, \"data\": data}\n",
    "        file_dump.append(summary)\n",
    "\n",
    "file_dump = sorted(file_dump, key=lambda i: i['date'])\n",
    "#if scraping date presented in descending order, use range(len(file_dump), 0)\n",
    "for item in file_dump:\n",
    "    print(\"Scraping date: %s    [downloaded_number_of_ads:%d]\"%(item['date'], item['number_ads']))\n",
    "    print(\"-----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": false,
    "tags": [
     "\"hide_output\""
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "<class 'str'>\n",
      "external_id\n",
      "<class 'str'>\n",
      "webpage_url\n",
      "<class 'str'>\n",
      "logo_url\n",
      "<class 'str'>\n",
      "headline\n",
      "<class 'str'>\n",
      "application_deadline\n",
      "<class 'str'>\n",
      "number_of_vacancies\n",
      "<class 'str'>\n",
      "description\n",
      "<class 'str'>\n",
      "---text---company_information---needs---requirements---conditions\n",
      "employment_type\n",
      "<class 'str'>\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "salary_type\n",
      "<class 'str'>\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "salary_description\n",
      "<class 'str'>\n",
      "duration\n",
      "<class 'str'>\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "working_hours_type\n",
      "<class 'str'>\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "scope_of_work\n",
      "<class 'str'>\n",
      "---min---max\n",
      "access\n",
      "<class 'str'>\n",
      "employer\n",
      "<class 'str'>\n",
      "---phone_number---email---url---organization_number---name---workplace\n",
      "application_details\n",
      "<class 'str'>\n",
      "---information---reference---email---via_af---url---other\n",
      "experience_required\n",
      "<class 'str'>\n",
      "access_to_own_car\n",
      "<class 'str'>\n",
      "driving_license_required\n",
      "<class 'str'>\n",
      "driving_license\n",
      "<class 'str'>\n",
      "occupation\n",
      "<class 'str'>\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "occupation_group\n",
      "<class 'str'>\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "occupation_field\n",
      "<class 'str'>\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "workplace_address\n",
      "<class 'str'>\n",
      "---municipality_code---municipality---region_code---region---country_code---country---street_address---postcode---city---coordinates\n",
      "must_have\n",
      "<class 'str'>\n",
      "---skills---languages---work_experiences\n",
      "nice_to_have\n",
      "<class 'str'>\n",
      "---skills---languages---work_experiences\n",
      "publication_date\n",
      "<class 'str'>\n",
      "last_publication_date\n",
      "<class 'str'>\n",
      "removed\n",
      "<class 'str'>\n",
      "removed_date\n",
      "<class 'str'>\n",
      "source_type\n",
      "<class 'str'>\n",
      "timestamp\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'simple_keys': ['id',\n",
       "  'external_id',\n",
       "  'webpage_url',\n",
       "  'logo_url',\n",
       "  'headline',\n",
       "  'application_deadline',\n",
       "  'number_of_vacancies',\n",
       "  'salary_description',\n",
       "  'access',\n",
       "  'experience_required',\n",
       "  'access_to_own_car',\n",
       "  'driving_license_required',\n",
       "  'driving_license',\n",
       "  'publication_date',\n",
       "  'last_publication_date',\n",
       "  'removed',\n",
       "  'removed_date',\n",
       "  'source_type',\n",
       "  'timestamp'],\n",
       " 'complex_keys': ['description',\n",
       "  'employment_type',\n",
       "  'salary_type',\n",
       "  'duration',\n",
       "  'working_hours_type',\n",
       "  'scope_of_work',\n",
       "  'employer',\n",
       "  'application_details',\n",
       "  'occupation',\n",
       "  'occupation_group',\n",
       "  'occupation_field',\n",
       "  'workplace_address',\n",
       "  'must_have',\n",
       "  'nice_to_have']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#functions to read in json keys and values into df\n",
    "#print out the json keys\n",
    "def print_keys(data, ifprint=False):\n",
    "    #data is a list of dictionaries, some key contain another dictionary\n",
    "    simple_keys = []\n",
    "    complex_keys = []\n",
    "    for k in (data[0].keys()):\n",
    "        if ifprint:\n",
    "            print(k)\n",
    "            print(type(k))\n",
    "        if isinstance(data[0][k], dict):\n",
    "            complex_keys.append(k)\n",
    "            if ifprint:\n",
    "                print(\"---%s\"%\"---\".join(data[0][k].keys()))\n",
    "        else:\n",
    "            simple_keys.append(k)\n",
    "    result = {'simple_keys': simple_keys, \"complex_keys\": complex_keys}\n",
    "    return result\n",
    "  \n",
    "            \n",
    "def get_keyvalue(data, index, keyname):\n",
    "    #return the value of the keyname at index in data\n",
    "    #return type is a string\n",
    "    if isinstance(data[index][keyname], dict):\n",
    "        return None\n",
    "    else:\n",
    "        return data[index][keyname]\n",
    "    \n",
    "def get_keyvalues(data, keyname):\n",
    "    #return a list values in data of the keyname\n",
    "    #only the simple keys\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if item.get(keyname) != None:\n",
    "            result.append(item[keyname])\n",
    "        else:\n",
    "            result.append(np.nan)\n",
    "    return result\n",
    "\n",
    "def get_commonstructure_type(data, ckeyname):\n",
    "    #with the given compley keyname\n",
    "    #return values in df, with each subkey as a column\n",
    "    #some complex keys share the same structure, i.e. the same subkeys\n",
    "    #create lists of subkeys\n",
    "    concept_id = []\n",
    "    label= []\n",
    "    legacy_ams_taxonomy_id = []\n",
    "    annons_id = get_keyvalues(data, 'id') #use this as key for matching back to other keys\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        #access this complex key as dictionary???\n",
    "        if data[i] is None:\n",
    "            legacy_ams_taxonomy_id.append(np.nan)\n",
    "            label.append(np.nan)\n",
    "            concept_id.append(np.nan)\n",
    "        else:\n",
    "            node= data[i].get(ckeyname)\n",
    "            if node != None:\n",
    "                if node.get('concept_id') != None:\n",
    "                    concept_id.append(node.get('concept_id'))\n",
    "                else:\n",
    "                    concept_id.append(np.nan)\n",
    "                if  node.get('label') != None:\n",
    "                    label.append(node.get('label'))\n",
    "                else:\n",
    "                    label.append(np.nan)\n",
    "                if node.get('legacy_ams_taxonomy_id') != None:\n",
    "                    legacy_ams_taxonomy_id.append(node.get('legacy_ams_taxonomy_id'))\n",
    "                else:\n",
    "                    legacy_ams_taxonomy_id.append(np.nan)\n",
    "\n",
    "            else:\n",
    "                legacy_ams_taxonomy_id.append(np.nan)\n",
    "                label.append(np.nan)\n",
    "                concept_id.append(np.nan)\n",
    "\n",
    "    result = pd.DataFrame({'%s_concept_id'%ckeyname: concept_id,\n",
    "                          '%s_label'%ckeyname: label,\n",
    "                          '%s_legacy_ams_taxonomy_id'%ckeyname: legacy_ams_taxonomy_id,\n",
    "                          'ads_id': annons_id})\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_ads_description(data, ckeyname='description'):\n",
    "    #with the given compley keyname\n",
    "    #return values in df, with each subkey as a column\n",
    "    #create lists of subkeys\n",
    "    text = []\n",
    "    company_info= []\n",
    "    needs = []\n",
    "    requirements = []\n",
    "    conditions = []\n",
    "    annons_id = get_keyvalues(data, 'id') #use ad key to match back to other keys\n",
    "\n",
    "    #get subkeys\n",
    "    subkeys = data[0].get(ckeyname).keys()\n",
    "    for i in range(len(data)):\n",
    "        if data[i] is None:\n",
    "            text.append(np.nan)\n",
    "            company_info.append(np.nan)\n",
    "            requirements.append(np.nan)\n",
    "            needs.append(np.nan)\n",
    "            conditions.append(np.nan)\n",
    "        else:   \n",
    "            description = data[i].get(ckeyname)\n",
    "            if description != None:\n",
    "            \n",
    "                if description.get('text') != None:\n",
    "                    text.append(description.get('text'))\n",
    "                else:\n",
    "                    text.append(np.nan)\n",
    "                if description.get('company_info') != None:\n",
    "                    company_info.append(description.get('company_info'))\n",
    "                else:\n",
    "                    company_info.append(np.nan)\n",
    "                if description.get('needs') != None:\n",
    "                    needs.append(description.get('needs'))\n",
    "                else:\n",
    "                    needs.append(np.nan)\n",
    "                if description.get('requirements') != None:\n",
    "                    requirements.append(description.get('requirements'))\n",
    "                else:\n",
    "                    requirements.append(np.nan)\n",
    "                if description.get('conditions') != None:\n",
    "                    conditions.append(description.get('conditions'))\n",
    "                else:\n",
    "                    conditions.append(np.nan)\n",
    "            else:\n",
    "                text.append(np.nan)\n",
    "                company_info.append(np.nan)\n",
    "                requirements.append(np.nan)\n",
    "                needs.append(np.nan)\n",
    "                conditions.append(np.nan)\n",
    "                \n",
    "    result = pd.DataFrame({'description_text': text,\n",
    "                          'description_company_info': company_info,\n",
    "                          'description_needs': needs,\n",
    "                          'description_requirements': requirements,\n",
    "                          'description_conditions': conditions,\n",
    "                          'ads_id': annons_id})\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_employer_values(data, ckeyname='employer'):\n",
    "    #with the given compley keyname\n",
    "    #return values in df, with each subkey as a column\n",
    "    #create lists of subkeys\n",
    "    #create lists of subkeys\n",
    "    phone = []\n",
    "    email = []\n",
    "    url = []\n",
    "    orgnr = []\n",
    "    name = []\n",
    "    workplace = [] #similar to company name\n",
    "    annons_id = get_keyvalues(data, 'id') #use ad key to match back to other keys\n",
    "\n",
    "    #get subkeys\n",
    "    subkeys = data[0].get(ckeyname).keys()\n",
    "    for i in range(len(data)):\n",
    "        if data[i] is None:\n",
    "            phone.append(np.nan)\n",
    "            email.append(np.nan)\n",
    "            url.append(np.nan)\n",
    "            orgnr.append(np.nan)\n",
    "            name.append(np.nan)\n",
    "            workplace.append(np.nan)\n",
    "        else:   \n",
    "            employer = data[i].get(ckeyname)\n",
    "            if employer != None:\n",
    "            \n",
    "                if employer.get('phone_number') != None:\n",
    "                    phone.append(employer.get('phone_number'))\n",
    "                else:\n",
    "                    phone.append(np.nan)\n",
    "                if employer.get('email') != None:\n",
    "                    email.append(employer.get('email'))\n",
    "                else:\n",
    "                    email.append(np.nan)\n",
    "                if employer.get('url') != None:\n",
    "                    url.append(employer.get('url'))\n",
    "                else:\n",
    "                    url.append(np.nan)\n",
    "                if employer.get('organization_number') != None:\n",
    "                    orgnr.append(employer.get('organization_number'))\n",
    "                else:\n",
    "                    orgnr.append(np.nan)\n",
    "                if employer.get('name') != None:\n",
    "                    name.append(employer.get('name'))\n",
    "                else:\n",
    "                    name.append(np.nan)\n",
    "                if employer.get('workplace') != None:\n",
    "                    workplace.append(employer.get('workplace'))\n",
    "                else:\n",
    "                    workplace.append(np.nan)\n",
    "            else:\n",
    "                phone.append(np.nan)\n",
    "                email.append(np.nan)\n",
    "                url.append(np.nan)\n",
    "                orgnr.append(np.nan)\n",
    "                name.append(np.nan)\n",
    "                workplace.append(np.nan)\n",
    "    result = pd.DataFrame({'employer_phone_number': phone,\n",
    "                          'employer_email': email,\n",
    "                          'employer_url': url,\n",
    "                          'employer_organization_number': orgnr,\n",
    "                          'employer_name': name,\n",
    "                          'employer_workplace': workplace,\n",
    "                          'ads_id': annons_id})\n",
    "    return result\n",
    "\n",
    "def get_work_addresses(jsondata, ckeyname=\"workplace_address\"):\n",
    "    #jsondata is a list of dictionaries\n",
    "    \n",
    "    municipality_code = []\n",
    "    municipality = []\n",
    "    region_code = []\n",
    "    region = []\n",
    "    country_code = []\n",
    "    country = []\n",
    "    street_address = []\n",
    "    postcode = []\n",
    "    city = []\n",
    "    coordinates = []\n",
    "    annons_id = get_keyvalues(jsondata, 'id')\n",
    "    \n",
    "    subkeys = jsondata[0].get(ckeyname).keys()\n",
    "    for i in range(len(jsondata)):\n",
    "        if jsondata[i] is None:\n",
    "            municipality_code.append(np.nan)\n",
    "            municipality.append(np.nan)\n",
    "            region_code.append(np.nan)\n",
    "            region.append(np.nan)\n",
    "            country_code.append(np.nan)\n",
    "            country.append(np.nan)\n",
    "            street_address.append(np.nan)\n",
    "            postcode.append(np.nan)\n",
    "            city.append(np.nan)\n",
    "            coordinates.append(np.nan)\n",
    "        else:\n",
    "            address = jsondata[i].get(ckeyname)\n",
    "            if address != None:\n",
    "                subkey_values = []\n",
    "                for j in subkeys:\n",
    "                    if address.get(j) != None:\n",
    "                        subkey_values.append(address.get(j))\n",
    "                    else:\n",
    "                        subkey_values.append(np.nan)\n",
    "            \n",
    "                municipality_code.append(subkey_values[0])\n",
    "                municipality.append(subkey_values[1])\n",
    "                region_code.append(subkey_values[2])\n",
    "                region.append(subkey_values[3])\n",
    "                country_code.append(subkey_values[4])\n",
    "                country.append(subkey_values[5])\n",
    "                street_address.append(subkey_values[6])\n",
    "                postcode.append(subkey_values[7])\n",
    "                city.append(subkey_values[8])\n",
    "                coordinates.append(subkey_values[9])    \n",
    "            else:\n",
    "                municipality_code.append(np.nan)\n",
    "                municipality.append(np.nan)\n",
    "                region_code.append(np.nan)\n",
    "                region.append(np.nan)\n",
    "                country_code.append(np.nan)\n",
    "                country.append(np.nan)\n",
    "                street_address.append(np.nan)\n",
    "                postcode.append(np.nan)\n",
    "                city.append(np.nan)\n",
    "                coordinates.append(np.nan)\n",
    "    result = pd.DataFrame({'address_municipality_code': municipality_code,\n",
    "                          'address_municipality': municipality,\n",
    "                          'address_region_code': region_code,\n",
    "                          'address_region': region,\n",
    "                          'address_country_code': country_code,\n",
    "                          'address_country': country,\n",
    "                          'address_street_address': street_address,\n",
    "                          'address_postcode': postcode,\n",
    "                          'address_city': city,\n",
    "                          'address_coordinates': coordinates,\n",
    "                           'ads_id': annons_id})\n",
    "    return result     \n",
    "                \n",
    "def convert_json2df(jsondata):\n",
    "    keys = print_keys(jsondata)\n",
    "    simple_keys = keys.get('simple_keys')\n",
    "    #print(simple_keys[0])\n",
    "    annons_id = get_keyvalues(jsondata, simple_keys[0])\n",
    "    #print(simple_keys[1])\n",
    "    external_id = get_keyvalues(jsondata, simple_keys[1])\n",
    "    #print(simple_keys[2])\n",
    "    webpage_url = get_keyvalues(jsondata, simple_keys[2])\n",
    "    #print(simple_keys[3])\n",
    "    logo_url = get_keyvalues(jsondata, simple_keys[3])\n",
    "    #print(simple_keys[4])\n",
    "    headline = get_keyvalues(jsondata, simple_keys[4])\n",
    "    #print(simple_keys[5])\n",
    "    application_deadline = get_keyvalues(jsondata, simple_keys[5])\n",
    "    #print(simple_keys[6])\n",
    "    number_of_vacancies = get_keyvalues(jsondata, simple_keys[6])\n",
    "    #print(simple_keys[7])\n",
    "    salary_description = get_keyvalues(jsondata, simple_keys[7])\n",
    "    #print(simple_keys[8])\n",
    "    access = get_keyvalues(jsondata, simple_keys[8])\n",
    "    #print(simple_keys[9])\n",
    "    experience_required=get_keyvalues(jsondata, simple_keys[9])\n",
    "    #print(print(simple_keys[10]))\n",
    "    access_to_own_car=get_keyvalues(jsondata, simple_keys[10])\n",
    "    #print(simple_keys[11])\n",
    "    driving_license_required=get_keyvalues(jsondata, simple_keys[11])\n",
    "    #print(simple_keys[12])\n",
    "    driving_license=get_keyvalues(jsondata, simple_keys[12])\n",
    "    #print(simple_keys[13])\n",
    "    publication_date=get_keyvalues(jsondata, simple_keys[13])\n",
    "    #print(simple_keys[14])\n",
    "    last_publication_date=get_keyvalues(jsondata, simple_keys[14])\n",
    "    #print(simple_keys[15])\n",
    "    removed=get_keyvalues(jsondata, simple_keys[15])\n",
    "    #print(simple_keys[16])\n",
    "    removed_date=get_keyvalues(jsondata, simple_keys[16])\n",
    "    #print(simple_keys[17])\n",
    "    source_type=get_keyvalues(jsondata, simple_keys[17])\n",
    "    #print(simple_keys[18])\n",
    "    timestamp=get_keyvalues(jsondata, simple_keys[18])\n",
    "    #convert data into df with a flat structure\n",
    "    df = pd.DataFrame()\n",
    "    df['ads_id'] = annons_id  #0\n",
    "    df['external_id'] = external_id #1\n",
    "    df['webpage_url'] = webpage_url #2\n",
    "    df['logo_url'] = logo_url #3\n",
    "    df['headline'] = headline #4\n",
    "    df['application_deadline'] = application_deadline #5\n",
    "    df['application_deadline'] = pd.to_datetime(df['application_deadline'])\n",
    "    df['number_of_vacancies'] = number_of_vacancies #6\n",
    "    df['salary_description'] = salary_description #7\n",
    "    df['access'] = access #8\n",
    "    df['experience_required'] = experience_required #9\n",
    "    df['access_to_own_car'] =access_to_own_car #10\n",
    "    df['driving_license_required'] = driving_license_required #11\n",
    "    df['driving_license'] = driving_license  #12\n",
    "    df['publication_date'] = publication_date  #13\n",
    "    df['publication_date'] = pd.to_datetime(df['publication_date']) \n",
    "    df['last_publication_date'] = last_publication_date  #14\n",
    "    df['last_pbulication_date'] = pd.to_datetime(df['last_publication_date'])\n",
    "    df['removed'] = removed  #15\n",
    "    df['removed_date'] = removed_date  #16\n",
    "    df['source_type'] = source_type  #17\n",
    "    df['timestamp'] = timestamp  #18\n",
    "    return df\n",
    "\n",
    "def add_dates2df(df):\n",
    "    #add year, month, week and day on according to publication date\n",
    "    #handle NaN in publication date\n",
    "    df['publication_date']=pd.to_datetime(df['publication_date'])\n",
    "    df['last_publication_date']=pd.to_datetime(df['last_publication_date'])\n",
    "    df['application_deadline']=pd.to_datetime(df['application_deadline'])\n",
    "    df['year']=df['publication_date'].apply(lambda x: np.nan if isinstance(x, pd._libs.tslibs.nattype.NaTType) else (x.strftime(\"%Y\"))) #4digits year\n",
    "    df['month']=df['publication_date'].apply(lambda x: np.nan if isinstance(x, pd._libs.tslibs.nattype.NaTType) else (x.strftime(\"%m\")))\n",
    "    df['weekday']=df['publication_date'].apply(lambda x: np.nan if isinstance(x, pd._libs.tslibs.nattype.NaTType) else str(x.isocalendar()[2]))\n",
    "    df['day']=df['publication_date'].apply(lambda x:np.nan if isinstance(x, pd._libs.tslibs.nattype.NaTType) else (x.strftime('%d')))\n",
    "    return df\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores = 8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def read_one_day_dump(json_dump):\n",
    "    #read one days data and add dates to the dataframe\n",
    "    #return the df\n",
    "    df = convert_json2df(json_dump)\n",
    "    df.drop_duplicates(subset = ['ads_id'], inplace=True)\n",
    "    df['removed'] = df['removed'].astype(bool)\n",
    "    df = df.loc[df.removed==False]\n",
    "    #add dates to the df\n",
    "    df = parallelize_dataframe(df, add_dates2df)\n",
    "    return df\n",
    "\n",
    "def add_more_data_dump(df1, df2):\n",
    "    #combine two days data_dumps and remove ads with \"True\"\n",
    "    #then drop duplicates\n",
    "    totaldf = pd.concat([df1, df2])\n",
    "    totaldf = totaldf.loc[totaldf['removed']==False]\n",
    "    totaldf.drop_duplicates(subset='ads_id', inplace = True)\n",
    "    return totaldf\n",
    "\n",
    "#print(\"After drop duplicates by ads_id: (%d, %d)\"%online_df.shape)\n",
    "print_keys(file_dump[0].get('data'), ifprint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#function that read one day json file at a time and create a time series for animation vacancies\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from celluloid import Camera\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "%matplotlib qt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.style.use('seaborn-poster')\n",
    "camera = Camera(fig)\n",
    "\n",
    "#summary is the df of the inread jsonfiles\n",
    "for i in range(len(file_dump)):\n",
    "    if i == 0:\n",
    "        total = read_one_day_dump(file_dump[i].get('data'))\n",
    "        #print(\"total: %d, %d\"%total.shape)\n",
    "        group_sum = total.groupby(['year', 'month'])['number_of_vacancies'].sum()\n",
    "        annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "        annonserdf['date']=annonserdf['date'].apply(lambda x: datetime.datetime.strptime(\"-\".join([x[0], x[1]]), \"%Y-%m\"))\n",
    "        annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        newdf = read_one_day_dump(file_dump[i].get('data'))\n",
    "        total = add_more_data_dump(total, newdf)\n",
    "        #print(\"total: %d, %d\"%total.shape)\n",
    "        group_sum = total.groupby(['year', 'month'])['number_of_vacancies'].sum()\n",
    "        annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "        annonserdf['date']=annonserdf['date'].apply(lambda x: datetime.datetime.strptime(\"-\".join([x[0], x[1]]), \"%Y-%m\"))\n",
    "        annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "    #print(annonserdf)\n",
    "    #f is a list of plt, reuse the same figure\n",
    "    f = plt.plot(annonserdf['date'], annonserdf['sum'], marker=\"*\")\n",
    "    plt.legend(f, [file_dump[i].get('date')])\n",
    "    plt.title(\"Platsbanken vacancies in month\")\n",
    "    plt.xlabel(\"Time(month)\")\n",
    "    plt.ylabel(\"Vacancies\")\n",
    "    plt.xticks(rotation=45)\n",
    "    camera.snap()\n",
    "\n",
    "\n",
    "ani = camera.animate(interval=1000)\n",
    "#gif file is a animation\n",
    "ani.save(\"/home/inlab4/Documents/AF/something.gif\", writer='imagemagick')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#read in two jsonfiles and compare the increasing number\n",
    "file_path1 = path+\"2020-02-11T08%3A45%3A16.txt\"\n",
    "file_path2 = path+\"2020-02-11.txt\"\n",
    "def find_increased(f1, f2):\n",
    "    # with the given loaded data jsonfile1 and jsonfile2 of two dates, comparing ads_id\n",
    "    #jsonfile2 loading date is late than jsonfile1\n",
    "    #return only the increaded\n",
    "    \n",
    "    with open(f1, 'r') as f:\n",
    "        data1 = json.load(f)\n",
    "    with open(f2, 'r') as f:\n",
    "        data2 = json.load(f)\n",
    "    df1 = convert_json2df(data1)\n",
    "    df2 = convert_json2df(data2)\n",
    "    online_df1 = df1.loc[df1.removed==False]\n",
    "    #use total data on date 2, since some may remove \n",
    "    merged_ads_outer = pd.merge(online_df1, df2, how='outer', on='ads_id')\n",
    "    merged_ads_inner = pd.merge(online_df1, df2, how='inner', on='ads_id')\n",
    "    increased = merged_ads_outer[merged_ads_outer.removed_y==False].shape[0]-merged_ads_inner.shape[0]\n",
    "    #how about increaed ads\n",
    "    return increased\n",
    "\n",
    "def get_increased_ads(f, date):\n",
    "    #by comparing giving a date, return ads that have bigger publication date\n",
    "    #f is the file downloaded on one day\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    df = convert_json2df(data)\n",
    "    result = df.loc[df.publication_date>=date]\n",
    "    return result\n",
    "\n",
    "#print(find_increased(file_path1, file_path2))\n",
    "#df = get_increased_ads(file_path2, datetime.date.fromisoformat('2020-02-10'))\n",
    "#df['publication_date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11562 entries, 0 to 11561\n",
      "Data columns (total 34 columns):\n",
      "ads_id                       11562 non-null object\n",
      "external_id                  8297 non-null object\n",
      "webpage_url                  11562 non-null object\n",
      "logo_url                     8865 non-null object\n",
      "headline                     11562 non-null object\n",
      "application_deadline         11562 non-null datetime64[ns]\n",
      "number_of_vacancies          11559 non-null float64\n",
      "salary_description           8060 non-null object\n",
      "access                       2 non-null object\n",
      "experience_required          11562 non-null object\n",
      "access_to_own_car            11562 non-null object\n",
      "driving_license_required     11562 non-null object\n",
      "driving_license              2496 non-null object\n",
      "publication_date             11562 non-null datetime64[ns]\n",
      "last_publication_date        11562 non-null datetime64[ns]\n",
      "last_pbulication_date        11562 non-null datetime64[ns]\n",
      "removed                      11562 non-null bool\n",
      "removed_date                 0 non-null object\n",
      "source_type                  11562 non-null object\n",
      "timestamp                    11562 non-null float64\n",
      "year                         11562 non-null object\n",
      "month                        11562 non-null object\n",
      "weekday                      11562 non-null object\n",
      "day                          11562 non-null object\n",
      "address_municipality_code    11271 non-null object\n",
      "address_municipality         11271 non-null object\n",
      "address_region_code          11486 non-null object\n",
      "address_region               11486 non-null object\n",
      "address_country_code         11562 non-null object\n",
      "address_country              11562 non-null object\n",
      "address_street_address       2233 non-null object\n",
      "address_postcode             2862 non-null object\n",
      "address_city                 2862 non-null object\n",
      "address_coordinates          11562 non-null object\n",
      "dtypes: bool(1), datetime64[ns](4), float64(2), object(27)\n",
      "memory usage: 3.0+ MB\n",
      "None\n",
      "[18.06858, 59.329323]                      1537\n",
      "[11.97456, 57.70887]                        714\n",
      "[None, None]                                606\n",
      "[13.003822, 55.60498]                       398\n",
      "[17.638927, 59.858562]                      187\n",
      "                                           ... \n",
      "[17.55958450430044, 59.092037074178585]       1\n",
      "[17.055823461614512, 59.33631666503721]       1\n",
      "[17.9434293545434, 59.3745395395914]          1\n",
      "[17.6882062307105, 59.9073995755831]          1\n",
      "[14.0441270492736, 56.298034634575]           1\n",
      "Name: address_coordinates, Length: 2262, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#show the number in bar or with map in the background???\n",
    "#first groupby municipality and then on nuts, finally draw in bars\n",
    "%matplotlib qt\n",
    "kommun_lan = pd.read_csv(\"/home/inlab4/Documents/Dan_datasets/kommun_lan.csv\", sep=\";\", dtype={\"Code\":str, \"nuts2\":str})\n",
    "#print(kommun_lan.head())\n",
    "def read_daily_data_address(fd):\n",
    "    #read in the data day by day with simple variabels + address\n",
    "    #fd is a dictionary structure created in the first cell, when the data were read in\n",
    "    total = read_one_day_dump(file_dump[0].get('data'))\n",
    "    #print(total.shape)\n",
    "    #we cannot see the column removed which is a simple variable, merge with total handle this\n",
    "    address = get_work_addresses(file_dump[0].get('data'))\n",
    "    #print(address.shape)\n",
    "    total = pd.merge(total, address, on='ads_id')\n",
    "    #print(total.describe())\n",
    "    return total\n",
    "\n",
    "totalad = pd.DataFrame()\n",
    "for i in range(len(file_dump)):\n",
    "    if i == 0:\n",
    "        totalad = read_daily_data_address(file_dump[i].get('data'))\n",
    "    else:\n",
    "        daydf = read_daily_data_address(file_dump[i].get('data'))\n",
    "        #add the new data\n",
    "        totalad = pd.concat([totalad, daydf])\n",
    "        totalad.drop_duplicates(subset=\"ads_id\", inplace=True)\n",
    "\n",
    "#check first null value in address_mulnicipality_code and \n",
    "print(totalad.info())\n",
    "print(totalad.address_coordinates.value_counts())\n",
    "#when all data are read in, use groupby and generate the region vacancies    \n",
    "#group_sum = totalad.groupby([\"address_municipality_code\", \"year\", \"month\"])['number_of_vacancies'].sum()\n",
    "#group_sum.sort_index(inplace=True)\n",
    "#annonserdf = pd.DataFrame()\n",
    "#annonserdf['municipality_code'] = group_sum.index.get_level_values(0)\n",
    "#annonserdf['date'] = [\"-\".join([y, m]) for (y,m) in zip(group_sum.index.get_level_values(1), group_sum.index.get_level_values(2))]\n",
    "#annonserdf['date'] = pd.to_datetime(annonserdf['date']) #this gives minutes seconds in the end of date\n",
    "#annonserdf['vacancies'] = group_sum.values\n",
    "#annonserdf = pd.merge(annonserdf, kommun_lan, left_on='municipality_code', right_on='Code')\n",
    "#print(annonserdf.info())\n",
    "#print(annonserdf)\n",
    "#nuts = ['SE11', 'SE12', 'SE21', 'SE22', 'SE23', 'SE31', 'SE32', 'SE33']\n",
    "#region_groups = annonserdf.groupby(['date', 'nuts2'])['vacancies'].sum().unstack('nuts2').fillna(0)\n",
    "\n",
    "#print(region_groups)\n",
    "#region_groups.plot(kind='bar', stacked=True) \n",
    "#plt.title(\"Total vacancies distributed in regions over Sweden\")\n",
    "#plt.xlabel(\"Time(month)\")\n",
    "#plt.ylabel(\"Vacancies\")\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import geopandas as gpd\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "countries = world[world['continent']=='Europe']\n",
    "sweden = countries[countries['name']=='Sweden']\n",
    "sweden.plot()\n",
    "print(sweden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---print the index----\n",
      "MultiIndex([('01', '2019', '10'),\n",
      "            ('01', '2019', '11'),\n",
      "            ('01', '2019', '12'),\n",
      "            ('01', '2020', '01'),\n",
      "            ('01', '2020', '02'),\n",
      "            ('03', '2019', '11'),\n",
      "            ('03', '2020', '01'),\n",
      "            ('03', '2020', '02'),\n",
      "            ('04', '2019', '11'),\n",
      "            ('04', '2020', '01'),\n",
      "            ('04', '2020', '02'),\n",
      "            ('05', '2019', '08'),\n",
      "            ('05', '2019', '12'),\n",
      "            ('05', '2020', '01'),\n",
      "            ('05', '2020', '02'),\n",
      "            ('06', '2019', '12'),\n",
      "            ('06', '2020', '01'),\n",
      "            ('06', '2020', '02'),\n",
      "            ('07', '2020', '01'),\n",
      "            ('07', '2020', '02'),\n",
      "            ('08', '2020', '01'),\n",
      "            ('08', '2020', '02'),\n",
      "            ('09', '2020', '01'),\n",
      "            ('09', '2020', '02'),\n",
      "            ('10', '2020', '02'),\n",
      "            ('12', '2020', '01'),\n",
      "            ('12', '2020', '02'),\n",
      "            ('13', '2019', '12'),\n",
      "            ('13', '2020', '01'),\n",
      "            ('13', '2020', '02'),\n",
      "            ('14', '2019', '10'),\n",
      "            ('14', '2019', '12'),\n",
      "            ('14', '2020', '01'),\n",
      "            ('14', '2020', '02'),\n",
      "            ('17', '2019', '12'),\n",
      "            ('17', '2020', '01'),\n",
      "            ('17', '2020', '02'),\n",
      "            ('18', '2020', '01'),\n",
      "            ('18', '2020', '02'),\n",
      "            ('19', '2020', '01'),\n",
      "            ('19', '2020', '02'),\n",
      "            ('20', '2020', '01'),\n",
      "            ('20', '2020', '02'),\n",
      "            ('21', '2019', '11'),\n",
      "            ('21', '2020', '01'),\n",
      "            ('21', '2020', '02'),\n",
      "            ('22', '2020', '01'),\n",
      "            ('22', '2020', '02'),\n",
      "            ('23', '2020', '01'),\n",
      "            ('23', '2020', '02'),\n",
      "            ('24', '2019', '12'),\n",
      "            ('24', '2020', '01'),\n",
      "            ('24', '2020', '02'),\n",
      "            ('25', '2019', '12'),\n",
      "            ('25', '2020', '01'),\n",
      "            ('25', '2020', '02'),\n",
      "            ('90', '2020', '01'),\n",
      "            ('90', '2020', '02')],\n",
      "           names=['address_region_code', 'year', 'month'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data '01-2019' does not match format '%Y-%m'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f0cf0c54b236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mannonserdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgroup_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgroup_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mannonserdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannonserdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y-%m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mannonserdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannonserdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f0cf0c54b236>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mannonserdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgroup_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgroup_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mannonserdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannonserdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y-%m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mannonserdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannonserdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    576\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 359\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n",
      "\u001b[0;31mValueError\u001b[0m: time data '01-2019' does not match format '%Y-%m'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     393395\n",
      "unique         1\n",
      "top        False\n",
      "freq      393395\n",
      "Name: removed, dtype: object\n",
      "count     62890\n",
      "unique        1\n",
      "top       False\n",
      "freq      62890\n",
      "Name: removed, dtype: object\n",
      "Total loaded online ads: 393395; Total after drop_duplicates: 62890; Duplicated ads are: 330505\n",
      "count     62890\n",
      "unique        1\n",
      "top       False\n",
      "freq      62890\n",
      "Name: removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df1 = convert_json2df(file_dump[0].get('data'))\n",
    "df2 = convert_json2df(file_dump[1].get('data'))\n",
    "df3 = convert_json2df(file_dump[2].get('data'))\n",
    "df4 = convert_json2df(file_dump[3].get('data'))\n",
    "df5 = convert_json2df(file_dump[4].get('data'))\n",
    "df6 = convert_json2df(file_dump[6].get('data'))\n",
    "df7 = convert_json2df(file_dump[7].get('data'))\n",
    "df8 = convert_json2df(file_dump[8].get('data'))\n",
    "df9 = convert_json2df(file_dump[9].get('data'))\n",
    "\n",
    "dfs = [df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "total = dfs[0].loc[dfs[0].removed==False]\n",
    "\n",
    "for i in range(1, len(dfs)):\n",
    "    #add only open ads\n",
    "    #print(\"data frame is: dfs%d\"%i)\n",
    "    #\"remove\" of some ads may tunr to True from False\n",
    "  \n",
    "    subset = dfs[i].loc[dfs[i].removed==False]\n",
    "    #print(subset['removed'].describe())\n",
    "    total = pd.concat([total,subset], ignore_index=True)\n",
    "    #print(total.shape)\n",
    "\n",
    "#print(total.head())\n",
    "before_ads = total.shape[0]\n",
    "print(total['removed'].describe())\n",
    "total.drop_duplicates(subset=['ads_id'], inplace=True)\n",
    "after_ads = total.shape[0]\n",
    "#only online ads are aggregated, check if this variable is unique False\n",
    "print(total['removed'].describe())\n",
    "diff_ads = before_ads-after_ads\n",
    "print(\"Total loaded online ads: %d; Total after drop_duplicates: %d; Duplicated ads are: %d\"%(before_ads, after_ads, diff_ads))\n",
    "print(total['removed'].describe())\n",
    "#download the file into csv for DB\n",
    "#write down the date of the aggregation, how to do this more efficiently???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62890, 24)\n",
      "     ads_id                        external_id  \\\n",
      "0  23840137               46-232100-0040-13796   \n",
      "1  23840142  46-556648-2781-0000007414DC0DDCA1   \n",
      "2  23802672                46-556662-0851-9059   \n",
      "3  23840140  46-556648-2781-0000007412F77DA615   \n",
      "4  23839717                                NaN   \n",
      "\n",
      "                                         webpage_url  \\\n",
      "0  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "1  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "2  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "3  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "4  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "\n",
      "                                            logo_url  \\\n",
      "0  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "1  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "2                                                NaN   \n",
      "3  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                            headline application_deadline  \\\n",
      "0   Sjuksköterska till Geriatrisk akutvårdsavdelning  2020-03-31 23:59:59   \n",
      "1         Jobba med kundservice hos Memira Eyecenter  2020-03-01 23:59:59   \n",
      "2  Extrajobb barnvakt 2-4 em/v Upplands-Väsby kol...  2020-03-07 23:59:59   \n",
      "3        Kundtjänst hos Memira Eyecenter i Stockholm  2020-03-01 23:59:59   \n",
      "4                         Butikspersonal Nisses Fisk  2020-05-15 23:59:59   \n",
      "\n",
      "   number_of_vacancies salary_description access experience_required  ...  \\\n",
      "0                  1.0                  .    NaN               False  ...   \n",
      "1                  1.0           Fast lön    NaN                True  ...   \n",
      "2                  1.0           Fast lön    NaN               False  ...   \n",
      "3                  1.0           Fast lön    NaN                True  ...   \n",
      "4                  2.0                NaN    NaN                True  ...   \n",
      "\n",
      "  last_publication_date last_pbulication_date removed removed_date  \\\n",
      "0   2020-03-31 23:59:59   2020-03-31 23:59:59   False          NaN   \n",
      "1   2020-03-01 23:59:59   2020-03-01 23:59:59   False          NaN   \n",
      "2   2020-03-07 23:59:59   2020-03-07 23:59:59   False          NaN   \n",
      "3   2020-03-01 23:59:59   2020-03-01 23:59:59   False          NaN   \n",
      "4   2020-05-15 23:59:59   2020-05-15 23:59:59   False          NaN   \n",
      "\n",
      "           source_type     timestamp  year month weekday  day  \n",
      "0  VIA_PLATSBANKEN_DXA  1.580811e+12  2020    02       2   04  \n",
      "1  VIA_PLATSBANKEN_DXA  1.580811e+12  2020    02       2   04  \n",
      "2  VIA_PLATSBANKEN_DXA  1.580811e+12  2020    02       2   04  \n",
      "3  VIA_PLATSBANKEN_DXA  1.580811e+12  2020    02       2   04  \n",
      "4        VIA_ANNONSERA  1.580811e+12  2020    02       2   04  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "execution time is  1.41\n",
      "Fast lön                                                                  6329\n",
      "Lön enligt överenskommelse                                                5806\n",
      "Månadslön                                                                 4673\n",
      "Enligt avtal.                                                             4230\n",
      "Enligt överenskommelse                                                    3696\n",
      "                                                                          ... \n",
      "Enligt kollektivavtal, Handels\\r\\nFriskvårdsbidrag                           1\n",
      "Lön enligt avtal Kommunal - KFO.\\r\\nFriskvårdsbidrag via Stil.               1\n",
      "Arbetsskor, Friskvårdsbidrag, Trivselpeng\\r\\nLön enligt kollektivavtal       1\n",
      " Ange löneanspråk\\nIndividuell lönesättning tillämpas.                       1\n",
      "Kommissionslön                                                               1\n",
      "Name: salary_description, Length: 1573, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "\n",
    "total = parallelize_dataframe(total, add_dates2df)\n",
    "print(total.shape)\n",
    "print(total.head())\n",
    "print(\"execution time is %5.2f\"%(time.process_time()-start))\n",
    "print(total['salary_description'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     7,
     12,
     34,
     44
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data shape is (62890, 24)\n",
      "Description of number of vacancies\n",
      "count    62813.000000\n",
      "mean         2.872781\n",
      "std         14.285241\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          2.000000\n",
      "max        909.000000\n",
      "Name: number_of_vacancies, dtype: float64\n",
      "(0, 24)\n",
      "*******************************************\n",
      "62890\n",
      "first publication date is : 2019-05-02 11:02:22\n",
      "last publication date is: 2020-02-19 18:58:04\n",
      "----------------Description of lasting days------------------\n",
      "count    62890.000000\n",
      "mean        41.277612\n",
      "std         40.174314\n",
      "min          1.000000\n",
      "25%         20.000000\n",
      "50%         30.000000\n",
      "75%         41.000000\n",
      "max        365.000000\n",
      "Name: lasting_days, dtype: float64\n",
      "--------Total ads have vacancies: 180448--------------\n",
      "Please write the maximum lasting days online of an adertisement allowed to be\n",
      "60\n",
      "ads lasting more than 60 days\n",
      "          ads_id           external_id  \\\n",
      "33760   23728284                   NaN   \n",
      "30177   23734687                   NaN   \n",
      "30317   23734698                   NaN   \n",
      "34189   23734361                   NaN   \n",
      "33884   23732815  46-202100-3153-13651   \n",
      "...          ...                   ...   \n",
      "393299  23884728                   NaN   \n",
      "393300  23884844                   NaN   \n",
      "393298  23884845                   NaN   \n",
      "393297  23884846                   NaN   \n",
      "393295  23884847    46-556451-9345-498   \n",
      "\n",
      "                                              webpage_url  \\\n",
      "33760   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "30177   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "30317   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "34189   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "33884   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "...                                                   ...   \n",
      "393299  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "393300  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "393298  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "393297  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "393295  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "\n",
      "                                                 logo_url  \\\n",
      "33760                                                 NaN   \n",
      "30177                                                 NaN   \n",
      "30317                                                 NaN   \n",
      "34189                                                 NaN   \n",
      "33884                                                 NaN   \n",
      "...                                                   ...   \n",
      "393299                                                NaN   \n",
      "393300                                                NaN   \n",
      "393298                                                NaN   \n",
      "393297                                                NaN   \n",
      "393295  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "\n",
      "                                                 headline  \\\n",
      "33760   Driftig krossoperatör som vill utvecklas med S...   \n",
      "30177   Fastighetsmäklare till Svensk Fastighetsförmed...   \n",
      "30317                                            SANERARE   \n",
      "34189    Förskollärare på Ströms Slott i Lilla Edet 100 %   \n",
      "33884          Doktorand i medicinsk vetenskap - Virologi   \n",
      "...                                                   ...   \n",
      "393299                                             Bovärd   \n",
      "393300  Vaccinationssjuksköterska till Sibyllekliniken...   \n",
      "393298                           Extrapersonal Hjulskifte   \n",
      "393297                           Extrapersonal Hjulskifte   \n",
      "393295                        Digital Worklife Strategist   \n",
      "\n",
      "       application_deadline  number_of_vacancies  \\\n",
      "33760   2020-02-11 23:59:59                  3.0   \n",
      "30177   2020-02-13 23:59:59                  1.0   \n",
      "30317   2020-02-14 23:59:59                  6.0   \n",
      "34189   2020-02-14 23:59:59                  2.0   \n",
      "33884   2020-02-13 23:59:59                  1.0   \n",
      "...                     ...                  ...   \n",
      "393299  2020-02-29 23:59:59                  1.0   \n",
      "393300  2020-02-29 23:59:59                  2.0   \n",
      "393298  2020-03-31 23:59:59                  1.0   \n",
      "393297  2020-03-31 23:59:59                  1.0   \n",
      "393295  2020-03-19 23:59:59                  1.0   \n",
      "\n",
      "                                       salary_description access  \\\n",
      "33760                                                 NaN    NaN   \n",
      "30177                                                 NaN    NaN   \n",
      "30317                                                 NaN    NaN   \n",
      "34189                                                 NaN    NaN   \n",
      "33884   Universitetet tillämpar lokalt avtal om lönesä...    NaN   \n",
      "...                                                   ...    ...   \n",
      "393299                                                NaN    NaN   \n",
      "393300                                                NaN    NaN   \n",
      "393298                                                NaN    NaN   \n",
      "393297                                                NaN    NaN   \n",
      "393295                                           Fast lön    NaN   \n",
      "\n",
      "       experience_required  ... last_pbulication_date removed removed_date  \\\n",
      "33760                 True  ...   2020-02-11 23:59:59   False          NaN   \n",
      "30177                 True  ...   2020-02-13 23:59:59   False          NaN   \n",
      "30317                 True  ...   2020-02-14 23:59:59   False          NaN   \n",
      "34189                 True  ...   2020-02-14 23:59:59   False          NaN   \n",
      "33884                 True  ...   2020-02-13 23:59:59   False          NaN   \n",
      "...                    ...  ...                   ...     ...          ...   \n",
      "393299                True  ...   2020-02-29 23:59:59   False          NaN   \n",
      "393300                True  ...   2020-02-29 23:59:59   False          NaN   \n",
      "393298                True  ...   2020-03-31 23:59:59   False          NaN   \n",
      "393297                True  ...   2020-03-31 23:59:59   False          NaN   \n",
      "393295                True  ...   2020-03-19 23:59:59   False          NaN   \n",
      "\n",
      "                source_type     timestamp  year  month weekday day  \\\n",
      "33760         VIA_ANNONSERA  1.576229e+12  2019     12       5  13   \n",
      "30177         VIA_ANNONSERA  1.578925e+12  2019     12       1  16   \n",
      "30317         VIA_ANNONSERA  1.578930e+12  2019     12       1  16   \n",
      "34189         VIA_ANNONSERA  1.576510e+12  2019     12       1  16   \n",
      "33884   VIA_PLATSBANKEN_DXA  1.576492e+12  2019     12       1  16   \n",
      "...                     ...           ...   ...    ...     ...  ..   \n",
      "393299        VIA_ANNONSERA  1.582135e+12  2020     02       3  19   \n",
      "393300        VIA_ANNONSERA  1.582135e+12  2020     02       3  19   \n",
      "393298        VIA_ANNONSERA  1.582135e+12  2020     02       3  19   \n",
      "393297        VIA_ANNONSERA  1.582135e+12  2020     02       3  19   \n",
      "393295  VIA_PLATSBANKEN_DXA  1.582135e+12  2020     02       3  19   \n",
      "\n",
      "        lasting_days  \n",
      "33760             60  \n",
      "30177             59  \n",
      "30317             60  \n",
      "34189             60  \n",
      "33884             59  \n",
      "...              ...  \n",
      "393299            10  \n",
      "393300            10  \n",
      "393298            41  \n",
      "393297            41  \n",
      "393295            29  \n",
      "\n",
      "[53043 rows x 25 columns]\n",
      "------ads lasting max 60 days have total vacancies: 120480------\n",
      "------ads lasting max 60 days have vacancies in 0.667672 of total vacancies\n",
      "------ads lasting max 60 days have 0.843425 of total ads\n"
     ]
    }
   ],
   "source": [
    "#show the description of the dataset\n",
    "online_df = total\n",
    "print(\"Total Data shape is (%d, %d)\"%online_df.shape)\n",
    "print(\"Description of number of vacancies\")\n",
    "print(online_df['number_of_vacancies'].describe())\n",
    "df_error = online_df.loc[online_df['number_of_vacancies'] == 0]\n",
    "print(df_error.shape)\n",
    "if df_error.shape[0]>0:\n",
    "    #print out text if some vacancies are registered in 0\n",
    "    print(\"Number of vacancies reported in 0: %d\"%df_error.shape[0])\n",
    "\n",
    "before = online_df['number_of_vacancies'].sum()\n",
    "if df_error.shape[0]>0: # if there are 0 in vacancies, handle it\n",
    "    online_df['number_of_vacancies'] = online_df['number_of_vacancies'].apply(lambda x: 1 if x==0 else (x))\n",
    "    print(\"after the controll of number of vacancies\")\n",
    "    total_vacancies = online_df['number_of_vacancies'].sum()\n",
    "    print(\"differences after check of number of vacancies: %d\"%(total_vacancies-before))\n",
    "\n",
    "#study the number of vacancies >10\n",
    "large_vacancies = online_df.loc[online_df['number_of_vacancies']>10]\n",
    "\n",
    "print(\"*******************************************\")\n",
    "#if 0 is identified in the dataset, do the following\n",
    "#online_df['number_of_vacancies'].fillna(0, inplace=True)   \n",
    "#online_df['number_of_vacancies'].describe()\n",
    "\n",
    "pdates = online_df['publication_date'].sort_values().tolist()\n",
    "print(len(pdates))\n",
    "print(\"first publication date is : %s\"%pdates[0])\n",
    "print(\"last publication date is: %s\"%pdates[-1])\n",
    "\n",
    "#is there any differences between last_pblucation_date and application_deadline??\n",
    "#create a list storing diff in days\n",
    "diff = []\n",
    "for row in online_df.itertuples():\n",
    "    diff_days = (row.last_publication_date - row.application_deadline).days\n",
    "    if diff_days>0:\n",
    "        diff.append(diff_days)\n",
    "if len(diff)>0:    \n",
    "    print(\"%d ads found differences between last_publication_date and application_deadline\"%len(diff))\n",
    "    print(diff)\n",
    "\n",
    "delta = []\n",
    "\n",
    "for row in online_df.itertuples():\n",
    "    delta.append((row.application_deadline-row.publication_date).days)\n",
    "    \n",
    "online_df['lasting_days'] = delta\n",
    "#print(online_df.head())\n",
    "#print(online_df.info())\n",
    "print(\"----------------Description of lasting days------------------\")\n",
    "print(online_df['lasting_days'].describe()) #can \n",
    "total_vacancies = online_df['number_of_vacancies'].sum()\n",
    "print(\"--------Total ads have vacancies: %d--------------\"%total_vacancies)\n",
    "print(\"Please write the maximum lasting days online of an adertisement allowed to be\")\n",
    "lasting_day = input(\"\")\n",
    "print(\"ads lasting more than %s days\"%lasting_day)\n",
    "current_ads = online_df.query('lasting_days<=%d'%int(lasting_day))\n",
    "print(current_ads.sort_values(by=['publication_date','lasting_days']))\n",
    "print(\"------ads lasting max %s days have total vacancies: %d------\"%(lasting_day, current_ads['number_of_vacancies'].sum()))\n",
    "print(\"------ads lasting max %s days have vacancies in %f of total vacancies\"%(lasting_day, current_ads['number_of_vacancies'].sum()/total_vacancies))\n",
    "print(\"------ads lasting max %s days have %f of total ads\"%(lasting_day, current_ads.shape[0]/online_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ads lasting less than 30 days has Vacancies percentage 0.47\n",
      "      ads_id                        external_id  \\\n",
      "1   23840142  46-556648-2781-0000007414DC0DDCA1   \n",
      "3   23840140  46-556648-2781-0000007412F77DA615   \n",
      "5   23840082                                NaN   \n",
      "6   23840145              46-556539-9747-315581   \n",
      "9   23840149              46-232100-0131-315486   \n",
      "10  23840150  46-556648-2781-0000007409D5FCA2EA   \n",
      "11  23840153  46-556648-2781-0000007410254D83D5   \n",
      "12  23840152                46-556271-1134-9841   \n",
      "13  23840154  46-556694-0044-0000001184BE97A825   \n",
      "14  23840155  46-556694-0044-0000001185D831C17E   \n",
      "\n",
      "                                          webpage_url  \\\n",
      "1   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "3   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "5   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "6   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "9   https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "10  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "11  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "12  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "13  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "14  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "\n",
      "                                             logo_url  \\\n",
      "1   https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "3   https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "5   https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "6                                                 NaN   \n",
      "9   https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "10  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "11  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "12  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "13  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "14  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "\n",
      "                                             headline application_deadline  \\\n",
      "1          Jobba med kundservice hos Memira Eyecenter  2020-03-01 23:59:59   \n",
      "3         Kundtjänst hos Memira Eyecenter i Stockholm  2020-03-01 23:59:59   \n",
      "5         AddUs Care söker sjuksköterska till Borås !  2020-03-05 23:59:59   \n",
      "6   Säljande receptionist heltid till Nordic Welln...  2020-02-18 23:59:59   \n",
      "9                                            Psykolog  2020-02-23 23:59:59   \n",
      "10    Dansktalande kundtjänst till Memira i Stockholm  2020-03-01 23:59:59   \n",
      "11    Norsktalande kundtjänst till Memira i Stockholm  2020-03-01 23:59:59   \n",
      "12                                            Provare  2020-03-01 23:59:59   \n",
      "13                             Product Safety Manager  2020-03-03 23:59:59   \n",
      "14                             Product Safety Manager  2020-03-03 23:59:59   \n",
      "\n",
      "    number_of_vacancies salary_description access experience_required  ...  \\\n",
      "1                   1.0           Fast lön    NaN                True  ...   \n",
      "3                   1.0           Fast lön    NaN                True  ...   \n",
      "5                   4.0                NaN    NaN                True  ...   \n",
      "6                   1.0           Fast lön    NaN               False  ...   \n",
      "9                   1.0          Månadslön    NaN                True  ...   \n",
      "10                  1.0           Fast lön    NaN                True  ...   \n",
      "11                  1.0           Fast lön    NaN                True  ...   \n",
      "12                  1.0           Fast lön    NaN                True  ...   \n",
      "13                  1.0           Fast lön    NaN                True  ...   \n",
      "14                  1.0           Fast lön    NaN                True  ...   \n",
      "\n",
      "   last_pbulication_date removed removed_date          source_type  \\\n",
      "1    2020-03-01 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "3    2020-03-01 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "5    2020-03-05 23:59:59   False          NaN        VIA_ANNONSERA   \n",
      "6    2020-02-18 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "9    2020-02-23 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "10   2020-03-01 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "11   2020-03-01 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "12   2020-03-01 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "13   2020-03-03 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "14   2020-03-03 23:59:59   False          NaN  VIA_PLATSBANKEN_DXA   \n",
      "\n",
      "       timestamp  year  month weekday day  lasting_days  \n",
      "1   1.580811e+12  2020     02       2  04            26  \n",
      "3   1.580811e+12  2020     02       2  04            26  \n",
      "5   1.580811e+12  2020     02       2  04            30  \n",
      "6   1.580811e+12  2020     02       2  04            14  \n",
      "9   1.580811e+12  2020     02       2  04            19  \n",
      "10  1.580811e+12  2020     02       2  04            26  \n",
      "11  1.580811e+12  2020     02       2  04            26  \n",
      "12  1.580811e+12  2020     02       2  04            26  \n",
      "13  1.580811e+12  2020     02       2  04            28  \n",
      "14  1.580811e+12  2020     02       2  04            28  \n",
      "\n",
      "[10 rows x 25 columns]\n",
      "outlier ads shape: (11223, 25)\n",
      "differences after the drop duplicates are 0 \n",
      "-------------Lasting days description-----------\n",
      "count    11223.000000\n",
      "mean       106.650183\n",
      "std         57.088074\n",
      "min          1.000000\n",
      "25%         72.000000\n",
      "50%        103.000000\n",
      "75%        170.000000\n",
      "max        365.000000\n",
      "Name: lasting_days, dtype: float64\n",
      "2221    2307\n",
      "2512    2285\n",
      "5343    1936\n",
      "3322    1647\n",
      "5223    1031\n",
      "        ... \n",
      "3153       1\n",
      "7115       1\n",
      "6222       1\n",
      "8312       1\n",
      "1540       1\n",
      "Name: occupation_group_legacy_ams_taxonomy_id, Length: 393, dtype: int64\n",
      "                                               ads_id  external_id  \\\n",
      "occupation_group_label                                               \n",
      "Mjukvaru- och systemutvecklare m.fl.              701          531   \n",
      "Grundutbildade sjuksköterskor                     498          389   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.     408          362   \n",
      "Personliga assistenter                            378          321   \n",
      "Kockar och kallskänkor                            226           89   \n",
      "Företagssäljare                                   221          185   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.       215           60   \n",
      "Restaurang- och köksbiträden m.fl.                178           91   \n",
      "Vårdare, boendestödjare                           150          129   \n",
      "Hovmästare och servitörer                         141           60   \n",
      "\n",
      "                                               webpage_url  logo_url  \\\n",
      "occupation_group_label                                                 \n",
      "Mjukvaru- och systemutvecklare m.fl.                   701        24   \n",
      "Grundutbildade sjuksköterskor                          498        45   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.          408        25   \n",
      "Personliga assistenter                                 378        33   \n",
      "Kockar och kallskänkor                                 226         5   \n",
      "Företagssäljare                                        221        13   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.            215         2   \n",
      "Restaurang- och köksbiträden m.fl.                     178         6   \n",
      "Vårdare, boendestödjare                                150        12   \n",
      "Hovmästare och servitörer                              141         5   \n",
      "\n",
      "                                               headline  application_deadline  \\\n",
      "occupation_group_label                                                          \n",
      "Mjukvaru- och systemutvecklare m.fl.                701                   701   \n",
      "Grundutbildade sjuksköterskor                       498                   498   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.       408                   408   \n",
      "Personliga assistenter                              378                   378   \n",
      "Kockar och kallskänkor                              226                   226   \n",
      "Företagssäljare                                     221                   221   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.         215                   215   \n",
      "Restaurang- och köksbiträden m.fl.                  178                   178   \n",
      "Vårdare, boendestödjare                             150                   150   \n",
      "Hovmästare och servitörer                           141                   141   \n",
      "\n",
      "                                               number_of_vacancies  \\\n",
      "occupation_group_label                                               \n",
      "Mjukvaru- och systemutvecklare m.fl.                           701   \n",
      "Grundutbildade sjuksköterskor                                  498   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.                  408   \n",
      "Personliga assistenter                                         377   \n",
      "Kockar och kallskänkor                                         226   \n",
      "Företagssäljare                                                221   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.                    215   \n",
      "Restaurang- och köksbiträden m.fl.                             178   \n",
      "Vårdare, boendestödjare                                        150   \n",
      "Hovmästare och servitörer                                      141   \n",
      "\n",
      "                                               salary_description  access  \\\n",
      "occupation_group_label                                                      \n",
      "Mjukvaru- och systemutvecklare m.fl.                          531       0   \n",
      "Grundutbildade sjuksköterskor                                 375       0   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.                 343       0   \n",
      "Personliga assistenter                                        303       0   \n",
      "Kockar och kallskänkor                                         78       4   \n",
      "Företagssäljare                                               183       0   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.                    60       1   \n",
      "Restaurang- och köksbiträden m.fl.                             88       2   \n",
      "Vårdare, boendestödjare                                       125       0   \n",
      "Hovmästare och servitörer                                      55       6   \n",
      "\n",
      "                                               experience_required  ...  \\\n",
      "occupation_group_label                                              ...   \n",
      "Mjukvaru- och systemutvecklare m.fl.                           701  ...   \n",
      "Grundutbildade sjuksköterskor                                  498  ...   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.                  408  ...   \n",
      "Personliga assistenter                                         378  ...   \n",
      "Kockar och kallskänkor                                         226  ...   \n",
      "Företagssäljare                                                221  ...   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.                    215  ...   \n",
      "Restaurang- och köksbiträden m.fl.                             178  ...   \n",
      "Vårdare, boendestödjare                                        150  ...   \n",
      "Hovmästare och servitörer                                      141  ...   \n",
      "\n",
      "                                               removed_date  source_type  \\\n",
      "occupation_group_label                                                     \n",
      "Mjukvaru- och systemutvecklare m.fl.                      0          701   \n",
      "Grundutbildade sjuksköterskor                             0          498   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.             0          408   \n",
      "Personliga assistenter                                    0          378   \n",
      "Kockar och kallskänkor                                    0          226   \n",
      "Företagssäljare                                           0          221   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.               0          215   \n",
      "Restaurang- och köksbiträden m.fl.                        0          178   \n",
      "Vårdare, boendestödjare                                   0          150   \n",
      "Hovmästare och servitörer                                 0          141   \n",
      "\n",
      "                                               timestamp  year  month  \\\n",
      "occupation_group_label                                                  \n",
      "Mjukvaru- och systemutvecklare m.fl.                 701   701    701   \n",
      "Grundutbildade sjuksköterskor                        498   498    498   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.        408   408    408   \n",
      "Personliga assistenter                               378   378    378   \n",
      "Kockar och kallskänkor                               226   226    226   \n",
      "Företagssäljare                                      221   221    221   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.          215   215    215   \n",
      "Restaurang- och köksbiträden m.fl.                   178   178    178   \n",
      "Vårdare, boendestödjare                              150   150    150   \n",
      "Hovmästare och servitörer                            141   141    141   \n",
      "\n",
      "                                               weekday  day  lasting_days  \\\n",
      "occupation_group_label                                                      \n",
      "Mjukvaru- och systemutvecklare m.fl.               701  701           701   \n",
      "Grundutbildade sjuksköterskor                      498  498           498   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.      408  408           408   \n",
      "Personliga assistenter                             378  378           378   \n",
      "Kockar och kallskänkor                             226  226           226   \n",
      "Företagssäljare                                    221  221           221   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.        215  215           215   \n",
      "Restaurang- och köksbiträden m.fl.                 178  178           178   \n",
      "Vårdare, boendestödjare                            150  150           150   \n",
      "Hovmästare och servitörer                          141  141           141   \n",
      "\n",
      "                                               occupation_group_concept_id  \\\n",
      "occupation_group_label                                                       \n",
      "Mjukvaru- och systemutvecklare m.fl.                                   701   \n",
      "Grundutbildade sjuksköterskor                                          498   \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.                          408   \n",
      "Personliga assistenter                                                 378   \n",
      "Kockar och kallskänkor                                                 226   \n",
      "Företagssäljare                                                        221   \n",
      "Eventsäljare och butiksdemonstratörer m.fl.                            215   \n",
      "Restaurang- och köksbiträden m.fl.                                     178   \n",
      "Vårdare, boendestödjare                                                150   \n",
      "Hovmästare och servitörer                                              141   \n",
      "\n",
      "                                               occupation_group_legacy_ams_taxonomy_id  \n",
      "occupation_group_label                                                                  \n",
      "Mjukvaru- och systemutvecklare m.fl.                                               701  \n",
      "Grundutbildade sjuksköterskor                                                      498  \n",
      "Undersköterskor, hemtjänst, äldreboende m.fl.                                      408  \n",
      "Personliga assistenter                                                             378  \n",
      "Kockar och kallskänkor                                                             226  \n",
      "Företagssäljare                                                                    221  \n",
      "Eventsäljare och butiksdemonstratörer m.fl.                                        215  \n",
      "Restaurang- och köksbiträden m.fl.                                                 178  \n",
      "Vårdare, boendestödjare                                                            150  \n",
      "Hovmästare och servitörer                                                          141  \n",
      "\n",
      "[10 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "#check duplicates: first check ads that have lasting days shorter than 30\n",
    "ads_less30 = online_df.loc[online_df['lasting_days']<=30]\n",
    "ads_less30_total = ads_less30.shape[0]\n",
    "ads_less30_vacancies = ads_less30['number_of_vacancies'].sum()\n",
    "#one outlier has lasting days as 365\n",
    "#print(data_left.loc[(data_left.year=='2019') & (data_left.month=='05')])\n",
    "\n",
    "print(\"Ads lasting less than 30 days has Vacancies percentage {:.2f}\".format(ads_less30_vacancies/total_vacancies))\n",
    "print(ads_less30.head(10))\n",
    "\n",
    "#domain_list = ads_less30['webpage_url'].apply(lambda x: x.split('/')[2])\n",
    "#site_list = ads_less30['webpage_url'].apply(lambda x: x.split('/')[-1])\n",
    "#print(len(set(domain_list)))\n",
    "#print(site_list[10:15])\n",
    "#ads_less30['webpage_url'].values.tolist()[2:5]\n",
    "\n",
    "outlier_ads = online_df.loc[(online_df['lasting_days']<7) | (online_df['lasting_days']>61)]\n",
    "print(\"outlier ads shape: (%d, %d)\"%outlier_ads.shape)\n",
    "before = outlier_ads.shape[0]\n",
    "outlier_ads = outlier_ads.drop_duplicates(subset=['ads_id', 'webpage_url'])\n",
    "print(\"differences after the drop duplicates are %d \"%(outlier_ads.shape[0]-before))\n",
    "print(\"-------------Lasting days description-----------\")\n",
    "print(outlier_ads['lasting_days'].describe())\n",
    "\n",
    "#merge with employment type and examine how the ads are distributed over the employment type\n",
    "employer = get_employer_values(file_dump[1].get('data'))\n",
    "employment_type = get_commonstructure_type(file_dump[1].get('data'), 'employment_type')\n",
    "duration = get_commonstructure_type(file_dump[1].get('data'), 'duration')\n",
    "#heltid, deltid\n",
    "working_hours_type = get_commonstructure_type(file_dump[1].get('data'), 'working_hours_type')\n",
    "occupation_group = get_commonstructure_type(file_dump[1].get('data'), 'occupation_group')\n",
    "salary_type = get_commonstructure_type(file_dump[1].get('data'), 'salary_type')\n",
    "\n",
    "#merge with employer and study how outerliers associate with employer\n",
    "#outlier_ads = outlier_ads.merge(employer, on='ads_id', how=\"left\")\n",
    "#print(outlier_ads['employer_workplace'].value_counts())\n",
    "#print(outlier_ads.groupby(['employer_organization_number']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "#print(outlier_ads[outlier_ads.employer_organization_number=='5590903570'])\n",
    "#print(outlier_ads.head(10))\n",
    "#print(outlier_ads.groupby(['experience_required']).count())\n",
    "#print(outlier_ads.groupby(['month']).count())\n",
    "\n",
    "#study the outlier with the employment type\n",
    "#outlier_ads = outlier_ads.merge(employment_type, on='ads_id', how=\"left\")\n",
    "#print(outlier_ads.groupby(['employment_type_label']).count().sort_values(by=\"ads_id\", ascending=False).head(10))\n",
    "#print(outlier_ads['employment_type_label'].value_counts())\n",
    "#outlier_ads = outlier_ads.merge(duration, on='ads_id',how=\"left\")\n",
    "#print(outlier_ads['duration_legacy_ams_taxonomy_id'].value_counts())\n",
    "#print(outlier_ads.shape)\n",
    "#print(outlier_ads.groupby(['duration_label']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "\n",
    "#outlier_ads = outlier_ads.merge(working_hours_type, on='ads_id', how=\"left\")\n",
    "#print(outlier_ads.groupby(['working_hours_type_label']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "#print(outlier_ads['working_hours_type_legacy_ams_taxonomy_id'].value_counts())\n",
    "outlier_ads = outlier_ads.merge(occupation_group, on='ads_id', how='left')\n",
    "print(occupation_group['occupation_group_legacy_ams_taxonomy_id'].value_counts())\n",
    "print(outlier_ads.groupby(['occupation_group_label']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "\n",
    "#online_ads = pd.merge(online_df, salary_type, on='ads_id', how='left')\n",
    "#print(online_ads.groupby(['salary_type_label']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "#print(online_ads['salary_description'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulization of totals vacancies\n",
    "print(\"Please write 'month' or 'day' for dawing total vacancies\")\n",
    "group_unit=input()\n",
    "\n",
    "if group_unit=='day':\n",
    "    #summary of daily vacancies\n",
    "    group_sum = online_ads.groupby(['year', 'month', 'day'])['number_of_vacancies'].sum()\n",
    "    #print(group_sum)\n",
    "    #convert a panda group multiindex into a list\n",
    "    annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "    annonserdf['date'] = annonserdf['date'].apply(lambda x: datetime.date.fromisoformat('-'.join([x[0], x[1], x[2]])))\n",
    "    annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "    print(annonserdf.head())\n",
    "    print(annonserdf.describe())\n",
    "    print(\"---check if some days have no vacancies---\")\n",
    "    print(annonserdf.loc[annonserdf['sum']==0.0])\n",
    "    ax = annonserdf.plot(x='date', y='sum', marker='*')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"/home/inlab4/Documents/AF/graphs/vacancies_daily.jpg\")\n",
    "elif group_unit=='month':\n",
    "    group_sum = online_ads.groupby(['year', 'month'])['number_of_vacancies'].sum()\n",
    "    #print(group_sum)\n",
    "    #convert a panda group multiindex into a list\n",
    "    annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "    annonserdf['date']=annonserdf['date'].apply(lambda x: datetime.datetime.strptime(\"-\".join([x[0], x[1]]), \"%Y-%m\"))\n",
    "    annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "    print(annonserdf.head())\n",
    "    print(annonserdf.describe())\n",
    "    print(\"---check if some days have no vacancies---\")\n",
    "    print(annonserdf.loc[annonserdf['sum']==0.0])\n",
    "    ax = annonserdf.plot(x='date', y='sum', marker='*')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"/home/inlab4/Documents/AF/graphs/vacancies_monthly.jpg\")\n",
    "else:\n",
    "    print(\"Please give a group unit in 'month' or 'day'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_text = []\n",
    "ansok_url = []\n",
    "import re\n",
    "urlpattern = re.compile('\\[http\\S\\]')\n",
    "for item in data:\n",
    "    if item.get('description') != None:\n",
    "        if item['description'].get('text') != None:\n",
    "            description_text.append(item['description']['text'])\n",
    "            \n",
    "            url = urlpattern.findall(item['description']['text'])\n",
    "                break\n",
    "            if url:\n",
    "                ansok_url.append(url[0])\n",
    "            else:\n",
    "                ansok_url.append('NaN')\n",
    "    else:\n",
    "        description_text.append('NaN')\n",
    "        ansok_url.append('NaN')\n",
    "\n",
    "print(len(description_text))\n",
    "print(description_text[100])\n",
    "print(ansok_url)\n",
    "s = \"39053\n",
    "KVALIFIKATIONER \n",
    "\n",
    "• Sjuksköterskeexamen med minst 1års yrkeserfarenhet som skolsköterska.\n",
    " • VUB inom område\n",
    " • Mycket goda kunskaper i Svenska, både i tal och skrift.\n",
    " • Meriterandemed erfarenhet avvaccination \n",
    "\n",
    "ANSÖKAN \n",
    "\n",
    "Du är varmt välkommen att skicka in din ansökan till oss. Ansök genom att klicka på knappen ”Ansök här”. Om du som sökande har frågor om den utannonserade tjänsten var vänlig och kontakta kontaktpersonen för denna annons. På vårhemsida [https://www.dedicare.se/yrkesroll/sjukskoterska/]presenterar vi alla våra lediga uppdrag. Urval och intervjuer sker löpande, vi tar tacksamt emot din ansökan snarast. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_id = []\n",
    "label= []\n",
    "legacy_ams_taxonomy_id = []\n",
    "\n",
    "for item in data1[0:3]:\n",
    "    employment_type = item.get('employment_type')\n",
    "    if isinstance(employment_type, dict):\n",
    "        concept_id.append(employment_type.get('concept_id'))\n",
    "        label.append(employment_type.get('label'))\n",
    "        legacy_ams_taxonomy_id.append(employment_type.get('legacy_ams_taxonomy_id'))\n",
    "\n",
    "    else:\n",
    "        concept_id.append(np.nan)\n",
    "        label.append(np.nan)\n",
    "        legacy_ams_taxonomy_id.append(np.nan)\n",
    "        \n",
    "print(concept_id)\n",
    "print(label)\n",
    "print(legacy_ams_taxonomy_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sum = data_left.groupby(['year', 'month'])['number_of_vacancies'].sum()\n",
    "#print(group_sum)\n",
    "#convert a panda group multiindex into a list\n",
    "annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "annonserdf['date']=annonserdf['date'].apply(lambda x: datetime.datetime.strptime(\"-\".join([x[0], x[1]]), \"%Y-%m\"))\n",
    "annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "print(annonserdf.head())\n",
    "print(annonserdf.describe())\n",
    "print(\"---check if some days have no vacancies---\")\n",
    "print(annonserdf.loc[annonserdf['sum']==0.0])\n",
    "ax = annonserdf.plot(x='date', y='sum', marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "434px",
    "left": "1448px",
    "right": "20px",
    "top": "119px",
    "width": "343px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
