{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:55:06.007195Z",
     "start_time": "2020-04-06T18:53:04.356094Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping date: 2020-02-07    [downloaded_number_of_ads:39053]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-11    [downloaded_number_of_ads:377412]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-12    [downloaded_number_of_ads:380087]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-13    [downloaded_number_of_ads:382552]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-14    [downloaded_number_of_ads:385766]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-15    [downloaded_number_of_ads:386129]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-16    [downloaded_number_of_ads:386901]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-17    [downloaded_number_of_ads:389594]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-18    [downloaded_number_of_ads:392733]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-19    [downloaded_number_of_ads:395434]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-20    [downloaded_number_of_ads:397847]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-21    [downloaded_number_of_ads:400860]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-22    [downloaded_number_of_ads:401192]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-23    [downloaded_number_of_ads:401746]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-24    [downloaded_number_of_ads:404169]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-25    [downloaded_number_of_ads:406989]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-26    [downloaded_number_of_ads:409427]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-27    [downloaded_number_of_ads:411663]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-28    [downloaded_number_of_ads:414521]\n",
      "-----------------------------------------\n",
      "Scraping date: 2020-02-29    [downloaded_number_of_ads:414564]\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Import libraries and processing the downloaded files to make them ready for DB\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import time\n",
    "import numpy\n",
    "from multiprocessing import Pool \n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "path = \"/home/inlab4/Documents/Dan_datasets/AF/feb/\"\n",
    "file_dump=[]\n",
    "#print(type(file_dump))\n",
    "#the file is saved in json format\n",
    "for f in glob.iglob(path+\"*.json\"):\n",
    "    with open(f, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        date = datetime.date.fromisoformat(f.split('/')[-1].replace(\".json\", \"\")[0:10])\n",
    "        number_ads = len(data)\n",
    "        summary = {\"date\": date, \"number_ads\": number_ads, \"file\": f, \"data\": data}\n",
    "        file_dump.append(summary)\n",
    "\n",
    "file_dump = sorted(file_dump, key=lambda i: i['date'])\n",
    "#if scraping date presented in descending order, use range(len(file_dump), 0)\n",
    "for item in file_dump:\n",
    "    print(\"Scraping date: %s    [downloaded_number_of_ads:%d]\"%(item['date'], item['number_ads']))\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T08:16:00.139134Z",
     "start_time": "2020-04-07T08:16:00.091015Z"
    },
    "code_folding": [
     150
    ],
    "hide_input": false,
    "tags": [
     "\"hide_output\""
    ]
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d39ea8c595e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;31m#print(\"After drop duplicates by ads_id: (%d, %d)\"%online_df.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m \u001b[0mprint_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dump\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mifprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#functions to read in json keys and values into df\n",
    "#print out the json keys\n",
    "import numpy\n",
    "def print_keys(data,ifprint=False):\n",
    "    #data is a list of dictionaries, some key contain another dictionary\n",
    "    #i is the index of the dictionary\n",
    "    simple_keys = []\n",
    "    complex_keys = []\n",
    "    for k in (data[0].keys()):\n",
    "        if isinstance(data[0][k], dict):\n",
    "            complex_keys.append(k)\n",
    "            if ifprint:\n",
    "                print(\"---%s\"%\"---\".join(data[0][k].keys()))\n",
    "        else:\n",
    "            simple_keys.append(k)\n",
    "            if ifprint:\n",
    "                print(k)\n",
    "                \n",
    "    result = {'simple_keys': simple_keys, \"complex_keys\": complex_keys}\n",
    "    return result\n",
    "\n",
    "            \n",
    "def get_keyvalue(data, index, keyname):\n",
    "    #return the value of the keyname at index in data\n",
    "    #return type is a string\n",
    "    if isinstance(data[index][keyname], dict):\n",
    "        return None\n",
    "    else:\n",
    "        return data[index][keyname]\n",
    "    \n",
    "def get_keyvalues(data, keyname):\n",
    "    #return a list values in data of the keyname\n",
    "    #only the simple keys\n",
    "    result = []\n",
    "    for item in data:\n",
    "        try: \n",
    "            if item.get(keyname) != None:\n",
    "                result.append(item[keyname])\n",
    "            else:\n",
    "                result.append(np.nan)\n",
    "        except IndexError as error:\n",
    "            #the key is not available\n",
    "            result.append(np.nan)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_commonstructure_type(data, ckeyname):\n",
    "    #with the given compley keyname\n",
    "    #return values in df, with each subkey as a column\n",
    "    #some complex keys share the same structure, i.e. the same subkeys\n",
    "    #create lists of subkeys\n",
    "    concept_id = []\n",
    "    label= []\n",
    "    legacy_ams_taxonomy_id = []\n",
    "    annons_id = get_keyvalues(data, 'id') #use this as key for matching back to other keys\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        #access this complex key as dictionary???\n",
    "        if data[i] is None:\n",
    "            legacy_ams_taxonomy_id.append(np.nan)\n",
    "            label.append(np.nan)\n",
    "            concept_id.append(np.nan)\n",
    "        else:\n",
    "            node= data[i].get(ckeyname)\n",
    "            if node != None:\n",
    "                if node.get('concept_id') != None:\n",
    "                    concept_id.append(node.get('concept_id'))\n",
    "                else:\n",
    "                    concept_id.append(np.nan)\n",
    "                if  node.get('label') != None:\n",
    "                    label.append(node.get('label'))\n",
    "                else:\n",
    "                    label.append(np.nan)\n",
    "                if node.get('legacy_ams_taxonomy_id') != None:\n",
    "                    legacy_ams_taxonomy_id.append(node.get('legacy_ams_taxonomy_id'))\n",
    "                else:\n",
    "                    legacy_ams_taxonomy_id.append(np.nan)\n",
    "\n",
    "            else:\n",
    "                legacy_ams_taxonomy_id.append(np.nan)\n",
    "                label.append(np.nan)\n",
    "                concept_id.append(np.nan)\n",
    "\n",
    "    result = pd.DataFrame({'%s_concept_id'%ckeyname: concept_id,\n",
    "                          '%s_label'%ckeyname: label,\n",
    "                          '%s_legacy_ams_taxonomy_id'%ckeyname: legacy_ams_taxonomy_id,\n",
    "                          'ads_id': annons_id})\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_ads_description(data, ckeyname='description'):\n",
    "    #with the given compley keyname\n",
    "    #return values in df, with each subkey as a column\n",
    "    #create lists of subkeys\n",
    "    text = []\n",
    "    company_info= []\n",
    "    needs = []\n",
    "    requirements = []\n",
    "    conditions = []\n",
    "    annons_id = get_keyvalues(data, 'id') #use ad key to match back to other keys\n",
    "\n",
    "    #get subkeys\n",
    "    subkeys = data[0].get(ckeyname).keys()\n",
    "    for i in range(len(data)):\n",
    "        if data[i] is None:\n",
    "            text.append(np.nan)\n",
    "            company_info.append(np.nan)\n",
    "            requirements.append(np.nan)\n",
    "            needs.append(np.nan)\n",
    "            conditions.append(np.nan)\n",
    "        else:   \n",
    "            description = data[i].get(ckeyname)\n",
    "            if description != None:\n",
    "            \n",
    "                if description.get('text') != None:\n",
    "                    text.append(description.get('text'))\n",
    "                else:\n",
    "                    text.append(np.nan)\n",
    "                if description.get('company_info') != None:\n",
    "                    company_info.append(description.get('company_info'))\n",
    "                else:\n",
    "                    company_info.append(np.nan)\n",
    "                if description.get('needs') != None:\n",
    "                    needs.append(description.get('needs'))\n",
    "                else:\n",
    "                    needs.append(np.nan)\n",
    "                if description.get('requirements') != None:\n",
    "                    requirements.append(description.get('requirements'))\n",
    "                else:\n",
    "                    requirements.append(np.nan)\n",
    "                if description.get('conditions') != None:\n",
    "                    conditions.append(description.get('conditions'))\n",
    "                else:\n",
    "                    conditions.append(np.nan)\n",
    "            else:\n",
    "                text.append(np.nan)\n",
    "                company_info.append(np.nan)\n",
    "                requirements.append(np.nan)\n",
    "                needs.append(np.nan)\n",
    "                conditions.append(np.nan)\n",
    "                \n",
    "    result = pd.DataFrame({'description_text': text,\n",
    "                          'description_company_info': company_info,\n",
    "                          'description_needs': needs,\n",
    "                          'description_requirements': requirements,\n",
    "                          'description_conditions': conditions,\n",
    "                          'ads_id': annons_id})\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_employer_values(data, ckeyname='employer'):\n",
    "    #with the given compley keyname\n",
    "    #return values in df, with each subkey as a column\n",
    "    #create lists of subkeys\n",
    "    #create lists of subkeys\n",
    "    phone = []\n",
    "    email = []\n",
    "    url = []\n",
    "    orgnr = []\n",
    "    name = []\n",
    "    workplace = [] #similar to company name\n",
    "    annons_id = get_keyvalues(data, 'id') #use ad key to match back to other keys\n",
    "\n",
    "    #get subkeys\n",
    "    subkeys = data[0].get(ckeyname).keys()\n",
    "    for i in range(len(data)):\n",
    "        if data[i] is None:\n",
    "            phone.append(np.nan)\n",
    "            email.append(np.nan)\n",
    "            url.append(np.nan)\n",
    "            orgnr.append(np.nan)\n",
    "            name.append(np.nan)\n",
    "            workplace.append(np.nan)\n",
    "        else:   \n",
    "            employer = data[i].get(ckeyname)\n",
    "            if employer != None:\n",
    "            \n",
    "                if employer.get('phone_number') != None:\n",
    "                    phone.append(employer.get('phone_number'))\n",
    "                else:\n",
    "                    phone.append(np.nan)\n",
    "                if employer.get('email') != None:\n",
    "                    email.append(employer.get('email'))\n",
    "                else:\n",
    "                    email.append(np.nan)\n",
    "                if employer.get('url') != None:\n",
    "                    url.append(employer.get('url'))\n",
    "                else:\n",
    "                    url.append(np.nan)\n",
    "                if employer.get('organization_number') != None:\n",
    "                    orgnr.append(employer.get('organization_number'))\n",
    "                else:\n",
    "                    orgnr.append(np.nan)\n",
    "                if employer.get('name') != None:\n",
    "                    name.append(employer.get('name'))\n",
    "                else:\n",
    "                    name.append(np.nan)\n",
    "                if employer.get('workplace') != None:\n",
    "                    workplace.append(employer.get('workplace'))\n",
    "                else:\n",
    "                    workplace.append(np.nan)\n",
    "            else:\n",
    "                phone.append(np.nan)\n",
    "                email.append(np.nan)\n",
    "                url.append(np.nan)\n",
    "                orgnr.append(np.nan)\n",
    "                name.append(np.nan)\n",
    "                workplace.append(np.nan)\n",
    "    result = pd.DataFrame({'employer_phone_number': phone,\n",
    "                          'employer_email': email,\n",
    "                          'employer_url': url,\n",
    "                          'employer_organization_number': orgnr,\n",
    "                          'employer_name': name,\n",
    "                          'employer_workplace': workplace,\n",
    "                          'ads_id': annons_id})\n",
    "    return result\n",
    "\n",
    "def get_work_addresses(jsondata, ckeyname=\"workplace_address\"):\n",
    "    #jsondata is a list of dictionaries\n",
    "    \n",
    "    municipality_code = []\n",
    "    municipality = []\n",
    "    region_code = []\n",
    "    region = []\n",
    "    country_code = []\n",
    "    country = []\n",
    "    street_address = []\n",
    "    postcode = []\n",
    "    city = []\n",
    "    coordinates = []\n",
    "    annons_id = get_keyvalues(jsondata, 'id')\n",
    "    \n",
    "    subkeys = jsondata[0].get(ckeyname).keys()\n",
    "    for i in range(len(jsondata)):\n",
    "        if jsondata[i] is None:\n",
    "            municipality_code.append(np.nan)\n",
    "            municipality.append(np.nan)\n",
    "            region_code.append(np.nan)\n",
    "            region.append(np.nan)\n",
    "            country_code.append(np.nan)\n",
    "            country.append(np.nan)\n",
    "            street_address.append(np.nan)\n",
    "            postcode.append(np.nan)\n",
    "            city.append(np.nan)\n",
    "            coordinates.append(np.nan)\n",
    "        else:\n",
    "            address = jsondata[i].get(ckeyname)\n",
    "            if address != None:\n",
    "                subkey_values = []\n",
    "                for j in subkeys:\n",
    "                    if address.get(j) != None:\n",
    "                        subkey_values.append(address.get(j))\n",
    "                    else:\n",
    "                        subkey_values.append(np.nan)\n",
    "            \n",
    "                municipality_code.append(subkey_values[0])\n",
    "                municipality.append(subkey_values[1])\n",
    "                region_code.append(subkey_values[2])\n",
    "                region.append(subkey_values[3])\n",
    "                country_code.append(subkey_values[4])\n",
    "                country.append(subkey_values[5])\n",
    "                street_address.append(subkey_values[6])\n",
    "                postcode.append(subkey_values[7])\n",
    "                city.append(subkey_values[8])\n",
    "                coordinates.append(subkey_values[9])    \n",
    "            else:\n",
    "                municipality_code.append(np.nan)\n",
    "                municipality.append(np.nan)\n",
    "                region_code.append(np.nan)\n",
    "                region.append(np.nan)\n",
    "                country_code.append(np.nan)\n",
    "                country.append(np.nan)\n",
    "                street_address.append(np.nan)\n",
    "                postcode.append(np.nan)\n",
    "                city.append(np.nan)\n",
    "                coordinates.append(np.nan)\n",
    "    result = pd.DataFrame({'address_municipality_code': municipality_code,\n",
    "                          'address_municipality': municipality,\n",
    "                          'address_region_code': region_code,\n",
    "                          'address_region': region,\n",
    "                          'address_country_code': country_code,\n",
    "                          'address_country': country,\n",
    "                          'address_street_address': street_address,\n",
    "                          'address_postcode': postcode,\n",
    "                          'address_city': city,\n",
    "                          'address_coordinates': coordinates,\n",
    "                           'ads_id': annons_id})\n",
    "    return result     \n",
    "\n",
    "def divide_json(json):\n",
    "    #remove items with removed==True the json and return the cleaned one\n",
    "    #re\n",
    "    loopnr = len(json)-1\n",
    "    result = []\n",
    "    for i in range(loopnr):\n",
    "        if len(json[i].keys()) != 3:\n",
    "            result.append(json[i])\n",
    "    return result\n",
    "                \n",
    "def convert_json2df(jsondata):\n",
    "    #api changed json data structure\n",
    "    simple_keys = ['id', 'external_id','webpage_url','logo_url','headline','application_deadline','number_of_vacancies','salary_description', 'access','experience_required','access_to_own_car','driving_license_required','driving_license','publication_date','last_publication_date','removed','removed_date','source_type','timestamp']\n",
    "    #keys in the dataframe\n",
    "    annons_id_removed = []\n",
    "    annons_id = []\n",
    "    external_id = []\n",
    "    webpage_url = []\n",
    "    logo_url= []\n",
    "    headline = []\n",
    "    application_deadline = []\n",
    "    number_of_vacancies = []\n",
    "    salary_description = []\n",
    "    access = []\n",
    "    experience_required = []\n",
    "    access_to_own_car = []\n",
    "    driving_license_required = []\n",
    "    driving_license = []\n",
    "    publication_date = []\n",
    "    last_publication_date = []\n",
    "    removed = []\n",
    "    removed_date = []\n",
    "    removed_removed = []\n",
    "    removed_date_removed = []\n",
    "    source_type = []\n",
    "    timestamp = []\n",
    "\n",
    "    annons_id = get_keyvalues(jsondata, simple_keys[0])\n",
    "    #print(simple_keys[1])\n",
    "    external_id = get_keyvalues(jsondata, simple_keys[1])\n",
    "    #print(simple_keys[2])\n",
    "    webpage_url = get_keyvalues(jsondata, simple_keys[2])\n",
    "    #print(simple_keys[3])\n",
    "    logo_url = get_keyvalues(jsondata, simple_keys[3])\n",
    "    #print(simple_keys[4])\n",
    "    headline = get_keyvalues(jsondata, simple_keys[4])\n",
    "    #print(simple_keys[5])\n",
    "    application_deadline = get_keyvalues(jsondata, simple_keys[5])\n",
    "    #print(simple_keys[6])\n",
    "    number_of_vacancies = get_keyvalues(jsondata, simple_keys[6])\n",
    "    #print(simple_keys[7])\n",
    "    salary_description = get_keyvalues(jsondata, simple_keys[7])\n",
    "    #print(simple_keys[8])\n",
    "    access = get_keyvalues(jsondata, simple_keys[8])\n",
    "    #print(simple_keys[9])\n",
    "    experience_required=get_keyvalues(jsondata, simple_keys[9])\n",
    "    #print(print(simple_keys[10]))\n",
    "    access_to_own_car=get_keyvalues(jsondata, simple_keys[10])\n",
    "    #print(simple_keys[11])\n",
    "    driving_license_required=get_keyvalues(jsondata, simple_keys[11])\n",
    "    #print(simple_keys[12])\n",
    "    driving_license=get_keyvalues(jsondata, simple_keys[12])\n",
    "    #print(simple_keys[13])\n",
    "    publication_date=get_keyvalues(jsondata, simple_keys[13])\n",
    "    #print(simple_keys[14])\n",
    "    last_publication_date=get_keyvalues(jsondata, simple_keys[14])\n",
    "    #print(simple_keys[15])\n",
    "    removed=get_keyvalues(jsondata, simple_keys[15])\n",
    "    #print(simple_keys[16])\n",
    "    removed_date=get_keyvalues(jsondata, simple_keys[16])\n",
    "    #print(simple_keys[17])\n",
    "    source_type=get_keyvalues(jsondata, simple_keys[17])\n",
    "    #print(simple_keys[18])\n",
    "    timestamp=get_keyvalues(jsondata, simple_keys[18])\n",
    "            #convert data into df with a flat structure\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['ads_id'] = annons_id  #0\n",
    "    df['external_id'] = external_id #1\n",
    "    df['webpage_url'] = webpage_url #2\n",
    "    df['logo_url'] = logo_url #3\n",
    "    df['headline'] = headline #4\n",
    "    df['application_deadline'] = application_deadline #5\n",
    "    df['number_of_vacancies'] = number_of_vacancies #6\n",
    "    df['number_of_vacancies'].astype(int, errors='ignore')\n",
    "    df['salary_description'] = salary_description #7\n",
    "    df['access'] = access #8\n",
    "    df['experience_required'] = experience_required #9\n",
    "    df['access_to_own_car'] =access_to_own_car #10\n",
    "    df['driving_license_required'] = driving_license_required #11\n",
    "    df['driving_license_required'].astype(numpy.bool)\n",
    "    df['driving_license'] = driving_license  #12\n",
    "    df['publication_date'] = publication_date  #13\n",
    "    df['last_publication_date'] = last_publication_date  #14\n",
    "    df['removed'] = removed  #15\n",
    "    df['removed'].astype(numpy.bool)\n",
    "    df['removed_date'] = removed_date  #16\n",
    "    df['source_type'] = source_type  #17\n",
    "    df['timestamp'] = timestamp  #18\n",
    "    return df\n",
    "\n",
    "def str2datetime(s, format=\"%Y-%m-%dT%H:%M:%S\"):\n",
    "    return datetime.datetime.strptime(s, format)\n",
    "\n",
    "def add_dates2df(df):\n",
    "    #add year, month, week and day on according to publication date\n",
    "    #handle NaN in publication date\n",
    "    df['year']=df['publication_date'].apply(lambda x: np.nan if isinstance(x, pd._libs.tslibs.nattype.NaTType) else (str2datetime(x).strftime(\"%Y\"))) #4digits year\n",
    "    df['month']=df['publication_date'].apply(lambda x: np.nan if isinstance(x, pd._libs.tslibs.nattype.NaTType) else (str2datetime(x).strftime(\"%m\")))\n",
    "    df['weekday']=df['publication_date'].apply(lambda x: np.nan if isinstance(x, pd._libs.tslibs.nattype.NaTType) else str(str2datetime(x).isocalendar()[2]))\n",
    "    df['day']=df['publication_date'].apply(lambda x:np.nan if isinstance(x, pd._libs.tslibs.nattype.NaTType) else (str2datetime(x).strftime('%d')))\n",
    "    return df\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores = 8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def read_one_day_dump(json_dump):\n",
    "    #read one days data and add dates to the dataframe\n",
    "    #return the df\n",
    "    df = convert_json2df(json_dump)\n",
    "    df.drop_duplicates(subset = ['ads_id'], inplace=True)\n",
    "    df['removed'] = df['removed'].astype(bool)\n",
    "    df = df.loc[df.removed==False]\n",
    "    #add dates to the df\n",
    "    df = parallelize_dataframe(df, add_dates2df)\n",
    "    return df\n",
    "\n",
    "def add_more_data_dump(df1, df2):\n",
    "    #combine two days data_dumps and remove ads with \"True\"\n",
    "    #then drop duplicates\n",
    "    totaldf = pd.concat([df1, df2])\n",
    "    totaldf = totaldf.loc[totaldf['removed']==False]\n",
    "    totaldf.drop_duplicates(subset='ads_id', inplace = True)\n",
    "    return totaldf\n",
    "\n",
    "#print(\"After drop duplicates by ads_id: (%d, %d)\"%online_df.shape)\n",
    "print_keys(file_dump[20].get('data'), ifprint=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T09:34:17.438585Z",
     "start_time": "2020-03-19T09:30:43.692773Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#create aggregated dataset for employer, address, ads_descriptions and save\n",
    "#them into files\n",
    "total_df = pd.DataFrame()\n",
    "for i in range(len(file_dump)):\n",
    "    #keep only json_records have all keys, delete those have removed\n",
    "    clean_json = divide_json(file_dump[i].get('data'))\n",
    "    #read the simple keys, only the online ads\n",
    "    df = read_one_day_dump(clean_json)\n",
    "    #read in the complex keys\n",
    "    #address_df = get_work_addresses(clean_json)\n",
    "    #description_df = get_ads_description(clean_json)\n",
    "    employer_df = get_employer_values(clean_json)\n",
    "    oneday_df = pd.merge(df, employer_df, how=\"left\", on='ads_id')##\n",
    "    #aggregate day by day data\n",
    "    total_df = add_more_data_dump(total_df, oneday_df)\n",
    "    print(f\"add {i} day: {df.shape[0]} rows\")\n",
    "    \n",
    "\n",
    "print(f\"total rows: {total_df.shape}\")\n",
    "file_name = \"Employer_df_2020-03-19.csv\"\n",
    "print(total_df.shape) #106914, 33\n",
    "total_df.to_csv(\"/home/inlab4/Documents/Dan_datasets/AF_results/%s\"%file_name)\n",
    "print(f\"saved {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T06:14:49.034622Z",
     "start_time": "2020-04-07T06:14:39.685317Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inlab4/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AEAnt  Sektor                          COAdress              Gata  \\\n",
      "0      0  111000             ADVOKATFIRMAN NOVA AB               BOX   \n",
      "1      0  111000  LINDSKOG MALMSTRÖM ADOKATBYRÅ KB               BOX   \n",
      "2      0  111000                               NaN     BETTORPSGATAN   \n",
      "3      0  111000        AMBER ADVOKATER VÄRNAMO HB               BOX   \n",
      "4      1  111000                BÄCKEGÅRDS LIST AB  BETARP BÄCKEGÅRD   \n",
      "\n",
      "       GatuNr    Ng1                                             Namn  \\\n",
      "0  55996       00000                            Legislatio Juridik AB   \n",
      "1  27707       00000                       Hårdvallsgatan Ventures AB   \n",
      "2  24 C        64202  Restaurangutrustnings Intressenter i Sverige AB   \n",
      "3  744         00000                          Tattoo Nation Sweden AB   \n",
      "4              00000                          Betarp Wood Products AB   \n",
      "\n",
      "       JE_orgnr    CfarNr       PeOrgNr  Ben             BGata     BGatuNr  \\\n",
      "0  165568361462       NaN           NaN  NaN               NaN         NaN   \n",
      "1  165568361470  50356401  165568361470                    NaN               \n",
      "2  165568361496       NaN           NaN  NaN               NaN         NaN   \n",
      "3  165568361504  58619909  165568361504  NaN               NaN               \n",
      "4  165568361546  54164470  165568361546  NaN  BETARP BÄCKEGÅRD               \n",
      "\n",
      "  BGatuRest  BPostNr   BPostOrt Kommun   BNg1  \n",
      "0       NaN      NaN        NaN    NaN    NaN  \n",
      "1       NaN      0.0  STOCKHOLM    180  86230  \n",
      "2       NaN      NaN        NaN    NaN    NaN  \n",
      "3       NaN      0.0    VÄRNAMO    683  96090  \n",
      "4       NaN  33378.0   BURSERYD    662  00000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inlab4/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    ads_id                        external_id  \\\n",
      "0           0  23840137               46-232100-0040-13796   \n",
      "1           1  23840142  46-556648-2781-0000007414DC0DDCA1   \n",
      "2           2  23802672                46-556662-0851-9059   \n",
      "3           3  23840140  46-556648-2781-0000007412F77DA615   \n",
      "4           4  23839717                                NaN   \n",
      "\n",
      "                                         webpage_url  \\\n",
      "0  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "1  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "2  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "3  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "4  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "\n",
      "                                            logo_url  \\\n",
      "0  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "1  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "2                                                NaN   \n",
      "3  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                            headline application_deadline  \\\n",
      "0   Sjuksköterska till Geriatrisk akutvårdsavdelning  2020-03-31T23:59:59   \n",
      "1         Jobba med kundservice hos Memira Eyecenter  2020-03-01T23:59:59   \n",
      "2  Extrajobb barnvakt 2-4 em/v Upplands-Väsby kol...  2020-03-07T23:59:59   \n",
      "3        Kundtjänst hos Memira Eyecenter i Stockholm  2020-03-01T23:59:59   \n",
      "4                         Butikspersonal Nisses Fisk  2020-05-15T23:59:59   \n",
      "\n",
      "   number_of_vacancies salary_description access  ...  \\\n",
      "0                  1.0                  .    NaN  ...   \n",
      "1                  1.0           Fast lön    NaN  ...   \n",
      "2                  1.0           Fast lön    NaN  ...   \n",
      "3                  1.0           Fast lön    NaN  ...   \n",
      "4                  2.0                NaN    NaN  ...   \n",
      "\n",
      "   address_municipality_code  address_municipality  address_region_code  \\\n",
      "0                       0581            Norrköping                   05   \n",
      "1                       0180             Stockholm                   01   \n",
      "2                       0114        Upplands Väsby                   01   \n",
      "3                       0180             Stockholm                   01   \n",
      "4                       0885              Borgholm                   08   \n",
      "\n",
      "      address_region address_country_code address_country  \\\n",
      "0  Östergötlands län                  199         Sverige   \n",
      "1     Stockholms län                  199         Sverige   \n",
      "2     Stockholms län                  199         Sverige   \n",
      "3     Stockholms län                  199         Sverige   \n",
      "4         Kalmar län                  199         Sverige   \n",
      "\n",
      "   address_street_address  address_postcode address_city  \\\n",
      "0                     NaN               NaN          NaN   \n",
      "1                     NaN               NaN          NaN   \n",
      "2                     NaN               NaN          NaN   \n",
      "3                     NaN               NaN          NaN   \n",
      "4    Per Lindströms väg 5             38735     Borgholm   \n",
      "\n",
      "                     address_coordinates  \n",
      "0                 [16.192421, 58.587746]  \n",
      "1                  [18.06858, 59.329323]  \n",
      "2                   [17.92834, 59.51961]  \n",
      "3                  [18.06858, 59.329323]  \n",
      "4  [16.6663838982659, 56.88004362252878]  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "   Unnamed: 0    ads_id                        external_id  \\\n",
      "0           0  23840137               46-232100-0040-13796   \n",
      "1           1  23840142  46-556648-2781-0000007414DC0DDCA1   \n",
      "2           2  23802672                46-556662-0851-9059   \n",
      "3           3  23840140  46-556648-2781-0000007412F77DA615   \n",
      "4           4  23839717                                NaN   \n",
      "\n",
      "                                         webpage_url  \\\n",
      "0  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "1  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "2  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "3  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "4  https://www.arbetsformedlingen.se/For-arbetsso...   \n",
      "\n",
      "                                            logo_url  \\\n",
      "0  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "1  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "2                                                NaN   \n",
      "3  https://www.arbetsformedlingen.se/rest/arbetsg...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                            headline application_deadline  \\\n",
      "0   Sjuksköterska till Geriatrisk akutvårdsavdelning  2020-03-31T23:59:59   \n",
      "1         Jobba med kundservice hos Memira Eyecenter  2020-03-01T23:59:59   \n",
      "2  Extrajobb barnvakt 2-4 em/v Upplands-Väsby kol...  2020-03-07T23:59:59   \n",
      "3        Kundtjänst hos Memira Eyecenter i Stockholm  2020-03-01T23:59:59   \n",
      "4                         Butikspersonal Nisses Fisk  2020-05-15T23:59:59   \n",
      "\n",
      "   number_of_vacancies salary_description access  ...     address_region  \\\n",
      "0                  1.0                  .    NaN  ...  Östergötlands län   \n",
      "1                  1.0           Fast lön    NaN  ...     Stockholms län   \n",
      "2                  1.0           Fast lön    NaN  ...     Stockholms län   \n",
      "3                  1.0           Fast lön    NaN  ...     Stockholms län   \n",
      "4                  2.0                NaN    NaN  ...         Kalmar län   \n",
      "\n",
      "   address_country_code  address_country address_street_address  \\\n",
      "0                   199          Sverige                    NaN   \n",
      "1                   199          Sverige                    NaN   \n",
      "2                   199          Sverige                    NaN   \n",
      "3                   199          Sverige                    NaN   \n",
      "4                   199          Sverige   Per Lindströms väg 5   \n",
      "\n",
      "  address_postcode address_city                    address_coordinates  \\\n",
      "0              NaN          NaN                 [16.192421, 58.587746]   \n",
      "1              NaN          NaN                  [18.06858, 59.329323]   \n",
      "2              NaN          NaN                   [17.92834, 59.51961]   \n",
      "3              NaN          NaN                  [18.06858, 59.329323]   \n",
      "4            38735     Borgholm  [16.6663838982659, 56.88004362252878]   \n",
      "\n",
      "                employer_url employer_organization_number  \\\n",
      "0  www.regionostergotland.se                   2321000040   \n",
      "1        https://www.tng.se/                   5566482781   \n",
      "2        http://www.nanny.nu                   5566620851   \n",
      "3        https://www.tng.se/                   5566482781   \n",
      "4          www.nissesfisk.se                   9165284150   \n",
      "\n",
      "                  employer_name  \n",
      "0  Östergötlands Läns Landsting  \n",
      "1                  Tng Group AB  \n",
      "2               2007 Nannynu AB  \n",
      "3                  Tng Group AB  \n",
      "4                Nisses Fisk HB  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only list-like objects are allowed to be passed to isin(), you passed a [str]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8ce37606d45a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0memployer_address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memployer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ads_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ads_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memployer_address\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0memployer_address\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memployer_address\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'employer_organization_number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   4512\u001b[0m         \u001b[0mName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0manimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4513\u001b[0m         \"\"\"\n\u001b[0;32m-> 4514\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4515\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(comps, values)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;34m\"only list-like objects are allowed to be passed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \" to isin(), you passed a [{values_type}]\".format(\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0mvalues_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             )\n\u001b[1;32m    440\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: only list-like objects are allowed to be passed to isin(), you passed a [str]"
     ]
    }
   ],
   "source": [
    "#analyse data and derive the cfarNr\n",
    "#read in data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fdb_dtypes = {'AEAnt': np.int32, 'COAdress': str, 'GatuNr': str, 'Ng1': str, 'PeOrgNr': str, 'JE_orgnr': str, 'CfarNr': str, 'BGatuNr': str, 'BpostNr': str, 'BNg1': str}\n",
    "fdb = pd.read_csv(r\"/media/inlab4/My Passport/Dan/fdb2018/JeAe2018.csv\", sep=\";\", dtype=fdb_dtypes)\n",
    "print(fdb.head())\n",
    "\n",
    "address_dtypes={'ads_id': str, 'address_municipality_code': str, 'address_region_code': str, 'address_country_code': str, 'address_postcode': str }\n",
    "address = pd.read_csv(\"/home/inlab4/Documents/Dan_datasets/AF_results/Address_df_2020-03-19.csv\", dtype=address_dtypes)\n",
    "print(address.head())\n",
    "employer_dtypes={'employer_organization_number': str, 'ads_id': str}\n",
    "employer = pd.read_csv(\"/home/inlab4/Documents/Dan_datasets/AF_results/Employer_df_2020-03-19.csv\", dtype=employer_dtypes)\n",
    "employer = employer[['ads_id', 'employer_url', 'employer_organization_number', 'employer_name']]\n",
    "employer_address = address.merge(employer, how=\"inner\", left_on=\"ads_id\", right_on=\"ads_id\")\n",
    "print(employer_address.head())\n",
    "employer_address[employer_address['employer_organization_number'].isin('')].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:11:27.027820Z",
     "start_time": "2020-03-19T15:11:26.267761Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(address.info()), work with AEAnt<=1\n",
    "#create files public_sector/Private_sector for the AEAnt<=1\n",
    "#create ads that AEAnt>1 and need more work on cfarnr\n",
    "fdb['JE_orgnr'] = fdb['PeOrgNr'].apply(lambda x: str(x)[2:12])\n",
    "print(fdb.shape)\n",
    "#fdb_1 = fdb.loc[fdb.AEAnt<=1]\n",
    "fdb_2 = fdb.loc[fdb.AEAnt>1] #97468\n",
    "orgnrs = fdb_2.JE_orgnr.values #97468\n",
    "print(fdb_1.shape)\n",
    "need_cfars = employer_address.loc[employer_address.employer_organization_number.isin(orgnrs)]\n",
    "print(need_cfars.shape) #60829\n",
    "#print(fdb.shape)#(2141285, 16)\n",
    "#print(fdb_1.head())#(1234001, 16)\n",
    "#print(pdf_2.shape)\n",
    "#cfar_1 = fdb_1.merge(employer_address, how=\"inner\", left_on='JE_orgnr', right_on='employer_organization_number')\n",
    "#print(cfar_1.shape) #52822\n",
    "#print(cfar_1.columns)\n",
    "#p_sect = ['131110', '131120', '131130', '131311', '131312', '131313', '131321', '131322', '131323', '131400']\n",
    "#public_sector = cfar_1.loc[cfar_1.Sektor.isin(p_sect)]\n",
    "#print(public_sector.shape)\n",
    "print(employer_address.shape)\n",
    "#public_sector.to_csv(\"/media/inlab4/My Passport/Dan/AF/public_sector_2020-03-19.csv\")\n",
    "#print(\"finished saving\" )\n",
    "#private_sector = cfar_1.loc[~cfar_1.Sektor.isin(p_sect)]\n",
    "#prv_sect = ['111000', '112000', '113000', '114000', '121000', '122100', '122200','122400', '122500', '125200','125300', '125900', '126100', '127000', '128100', '128200', '128300', '129100', '129300', '129400', '141000', '142000']\n",
    "#private_sector = cfar_1.loc[cfar_1.Sektor.isin(prv_sect)]\n",
    "#print(private_sector.head())\n",
    "#print(private_sector.shape) #51528\n",
    "#private_sector.to_csv(\"/media/inlab4/My Passport/Dan/AF/private_sector:_2020-03-19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:54:42.406303Z",
     "start_time": "2020-04-06T19:53:07.381060Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------2020-02-07----------\n",
      "        date    sum\n",
      "0 2019-08-01    1.0\n",
      "1 2019-10-01    3.0\n",
      "2 2019-11-01    4.0\n",
      "3 2019-12-01   26.0\n",
      "4 2020-01-01  498.0\n",
      "-----------2020-02-11----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-12----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-13----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-14----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-15----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-16----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-17----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-18----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-19----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-20----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-21----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-22----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-23----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-24----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-25----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-26----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-27----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-28----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n",
      "-----------2020-02-29----------\n",
      "        date     sum\n",
      "0 2019-05-01     1.0\n",
      "1 2019-08-01    77.0\n",
      "2 2019-09-01   493.0\n",
      "3 2019-10-01  1175.0\n",
      "4 2019-11-01  5128.0\n"
     ]
    }
   ],
   "source": [
    "#function that read one day json file at a time and create a time series for animation vacancies\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from celluloid import Camera\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.style.use('seaborn-poster')\n",
    "camera = Camera(fig)\n",
    "loops = len(file_dump)\n",
    "#summary is the df of the inread jsonfiles\n",
    "for i in range(loops):\n",
    "    if i == 0:\n",
    "        total = read_one_day_dump(file_dump[i].get('data'))\n",
    "        print(f\"-----------{file_dump[i].get('date')}----------\")\n",
    "        group_sum = total.groupby(['year', 'month'])['number_of_vacancies'].sum()\n",
    "        annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "        annonserdf['date']=annonserdf['date'].apply(lambda x: datetime.datetime.strptime(\"-\".join([x[0], x[1]]), \"%Y-%m\"))\n",
    "        annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "        print(annonserdf)\n",
    "        #days.append(annonserdf['date'].values.tolist())\n",
    "        #sums.append(annonserdf['sum'].values.tolist())\n",
    "        df_lists.append(annonserdf)\n",
    "        \n",
    "    else:\n",
    "        #print(f\"working on {i} file_dump\")\n",
    "        newdf = pd.DataFrame()\n",
    "        newdf = read_one_day_dump(file_dump[i].get('data'))\n",
    "        total = add_more_data_dump(total, newdf)\n",
    "        print(f\"-----------{file_dump[i].get('date')}----------\")\n",
    "        group_sum = total.groupby(['year', 'month'])['number_of_vacancies'].sum()\n",
    "        annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "        annonserdf['date']=annonserdf['date'].apply(lambda x: datetime.datetime.strptime(\"-\".join([x[0], x[1]]), \"%Y-%m\"))\n",
    "        annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "        #days.append(annonserdf['date'].values.tolist())\n",
    "        #sums.append(annonserdf['sum'].values.tolist())\n",
    "        print(annonserdf)\n",
    "        df_lists.append(annonserdf)\n",
    "    #print(annonserdf)\n",
    "    #f is a list of plt, reuse the same figure\n",
    "    f = plt.plot(annonserdf['date'], annonserdf['sum'], marker=\"*\")\n",
    "    plt.legend(f, [file_dump[i].get('date')])\n",
    "    plt.title(\"Platsbanken vacancies in month\")\n",
    "    plt.xlabel(\"Time(month)\")\n",
    "    plt.ylabel(\"Vacancies\")\n",
    "    plt.xticks(rotation=45)\n",
    "    camera.snap()\n",
    "\n",
    "#interval define how fast the animation shows\n",
    "ani = camera.animate(interval=1000)\n",
    "#gif file is a animation\n",
    "#ani.save(\"/home/inlab4/Documents/AF/feb.gif\", writer='imagemagick')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T06:28:20.248850Z",
     "start_time": "2020-04-07T06:28:20.239873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2019-05-01 00:00:00'), Timestamp('2019-08-01 00:00:00'), Timestamp('2019-09-01 00:00:00'), Timestamp('2019-10-01 00:00:00'), Timestamp('2019-11-01 00:00:00'), Timestamp('2019-12-01 00:00:00'), Timestamp('2020-01-01 00:00:00'), Timestamp('2020-02-01 00:00:00')]\n",
      "defaultdict(<class 'list'>, {Timestamp('2019-08-01 00:00:00'): [1.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0, 77.0], Timestamp('2019-10-01 00:00:00'): [3.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0, 1175.0], Timestamp('2019-11-01 00:00:00'): [4.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0, 5128.0], Timestamp('2019-12-01 00:00:00'): [26.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0, 19883.0], Timestamp('2020-01-01 00:00:00'): [498.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0, 73303.0], Timestamp('2020-02-01 00:00:00'): [24695.0, 46476.0, 51507.0, 56040.0, 61646.0, 63380.0, 64385.0, 71443.0, 76087.0, 80405.0, 84246.0, 89452.0, 90163.0, 90621.0, 98257.0, 102267.0, 106837.0, 110330.0, 116029.0, 116292.0], Timestamp('2019-05-01 00:00:00'): [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Timestamp('2019-09-01 00:00:00'): [493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0, 493.0]})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "vacancies_bydays = defaultdict(list) #total of each month\n",
    "dates = []\n",
    "days = []\n",
    "differences = defaultdict(list)\n",
    "for df in df_lists:\n",
    "    for item in df.itertuples():\n",
    "        if item.date not in dates:\n",
    "            dates.append(item.date)\n",
    "            vacancies_bydays.setdefault(item.date, []).append(item.sum)\n",
    "            \n",
    "        else:\n",
    "            vacancies_bydays[item.date].append(item.sum)\n",
    "            \n",
    "print(sorted(dates))\n",
    "print(vacancies)\n",
    "for d in range(7, 29):\n",
    "    days.append(datetime.date(2020, 2, d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T08:36:17.069705Z",
     "start_time": "2020-04-07T08:36:17.042762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "external_id\n",
      "webpage_url\n",
      "logo_url\n",
      "headline\n",
      "application_deadline\n",
      "number_of_vacancies\n",
      "---text---company_information---needs---requirements---conditions\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "salary_description\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---min---max\n",
      "access\n",
      "---phone_number---email---url---organization_number---name---workplace\n",
      "---information---reference---email---via_af---url---other\n",
      "experience_required\n",
      "access_to_own_car\n",
      "driving_license_required\n",
      "driving_license\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---municipality_code---municipality---region_code---region---country_code---country---street_address---postcode---city---coordinates\n",
      "---skills---languages---work_experiences\n",
      "---skills---languages---work_experiences\n",
      "publication_date\n",
      "last_publication_date\n",
      "removed\n",
      "removed_date\n",
      "source_type\n",
      "timestamp\n",
      "id\n",
      "external_id\n",
      "webpage_url\n",
      "logo_url\n",
      "headline\n",
      "application_deadline\n",
      "number_of_vacancies\n",
      "---text---company_information---needs---requirements---conditions\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "salary_description\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---min---max\n",
      "access\n",
      "---phone_number---email---url---organization_number---name---workplace\n",
      "---information---reference---email---via_af---url---other\n",
      "experience_required\n",
      "access_to_own_car\n",
      "driving_license_required\n",
      "driving_license\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---concept_id---label---legacy_ams_taxonomy_id\n",
      "---municipality_code---municipality---region_code---region---country_code---country---street_address---postcode---city---coordinates\n",
      "---skills---languages---work_experiences\n",
      "---skills---languages---work_experiences\n",
      "publication_date\n",
      "last_publication_date\n",
      "removed\n",
      "removed_date\n",
      "source_type\n",
      "timestamp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'simple_keys': ['id',\n",
       "  'external_id',\n",
       "  'webpage_url',\n",
       "  'logo_url',\n",
       "  'headline',\n",
       "  'application_deadline',\n",
       "  'number_of_vacancies',\n",
       "  'salary_description',\n",
       "  'access',\n",
       "  'experience_required',\n",
       "  'access_to_own_car',\n",
       "  'driving_license_required',\n",
       "  'driving_license',\n",
       "  'publication_date',\n",
       "  'last_publication_date',\n",
       "  'removed',\n",
       "  'removed_date',\n",
       "  'source_type',\n",
       "  'timestamp'],\n",
       " 'complex_keys': ['description',\n",
       "  'employment_type',\n",
       "  'salary_type',\n",
       "  'duration',\n",
       "  'working_hours_type',\n",
       "  'scope_of_work',\n",
       "  'employer',\n",
       "  'application_details',\n",
       "  'occupation',\n",
       "  'occupation_group',\n",
       "  'occupation_field',\n",
       "  'workplace_address',\n",
       "  'must_have',\n",
       "  'nice_to_have']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_keys(file_dump[10].get('data'), ifprint=True)\n",
    "print_keys(file_dump[11].get('data'), ifprint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T14:21:26.790373Z",
     "start_time": "2020-03-13T14:19:44.998092Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use a reference_month for saving ads that remain online after the publication date passed the same month\n",
    "#i.e. comparing month(publication_date) and month(last_publication_date)\n",
    "#the result will be a dataframe move intoto the DB, including orgnr, ads_id, publication_date, last_publication_date\n",
    "#reference_month and vacancies\n",
    "\n",
    "#print(df.shape)\n",
    "#print(df.head())\n",
    "#print(df_removed.shape)\n",
    "#print(df_removed.head())\n",
    "\n",
    "def calculate_vacancies(df):\n",
    "    #check the publication date, last_publication_date and generate newdf\n",
    "    #with reference_month\n",
    "    #delete removed ads\n",
    "    errors = []\n",
    "    #keep online data\n",
    "    df = df.loc[df['removed'] == False]\n",
    "    df.drop_duplicates(subset='ads_id', inplace = True)\n",
    "    new_df = pd.DataFrame()\n",
    "    new_ads_id = []\n",
    "    new_number_of_vacancies = []\n",
    "    new_publication_date=[]\n",
    "    new_last_publication_date=[]\n",
    "    new_timestamp=[]\n",
    "    reference_month = []\n",
    "    added_rows = 0\n",
    "    for row in df.itertuples():\n",
    "        try: #should be 100% with data, but in case\n",
    "            publication_date = str2datetime(str(row.publication_date))\n",
    "            publication_year = publication_date.strftime(\"%Y\")\n",
    "            last_publication_date = str2datetime(str(row.last_publication_date))\n",
    "            last_publication_year = last_publication_date.strftime(\"%Y\")\n",
    "            #If the two dates cover different year, throw the ads\n",
    "            if publication_year == last_publication_year:\n",
    "                publication_month =  publication_date.strftime(\"%m\")\n",
    "                last_month = last_publication_date.strftime(\"%m\")\n",
    "                if int(publication_month)<int(last_month):\n",
    "                    duplicates = (int(last_month)-int(publication_month)) +1\n",
    "                    added_rows = added_rows + (duplicates - 1)\n",
    "                    for i in range(duplicates):\n",
    "                        new_ads_id.append(row.ads_id)\n",
    "                        new_publication_date.append(publication_date)\n",
    "                        new_last_publication_date.append(last_publication_date)\n",
    "                        new_timestamp.append(row.timestamp)\n",
    "                        new_number_of_vacancies.append(row.number_of_vacancies)\n",
    "                        the_reference_month = int(publication_month) + i\n",
    "                        reference_month.append(str(the_reference_month))\n",
    "                \n",
    "                elif publication_month == last_month:\n",
    "                    #added the original row\n",
    "                    new_ads_id.append(row.ads_id)\n",
    "                    new_publication_date.append(publication_date)\n",
    "                    new_last_publication_date.append(last_publication_date)\n",
    "                    new_timestamp.append(row.timestamp)\n",
    "                    new_number_of_vacancies.append(row.number_of_vacancies)\n",
    "                    reference_month.append(int(publication_month))\n",
    "                else:\n",
    "                    error = publication_date - last_publication_date\n",
    "                    print(f\"ads_id: {row.ads_id} contains {error}\")\n",
    "        except:\n",
    "            #record the error row\n",
    "            errors.append(row)\n",
    "\n",
    "    new_df['ads_id'] = new_ads_id\n",
    "    new_df['number_of_vacancies'] = new_number_of_vacancies\n",
    "    new_df['publication_date'] = new_publication_date\n",
    "    new_df['last_publication_date'] = new_last_publication_date\n",
    "    new_df['timestamp'] = new_timestamp\n",
    "    new_df['reference_month'] = reference_month\n",
    "    print(f\"added rows {added_rows}\")\n",
    "    print(df.shape)\n",
    "    print(new_df.shape)\n",
    "    return new_df\n",
    "\n",
    "totaldf = pd.DataFrame()\n",
    "processed_file = 0\n",
    "for i in range(len(file_dump)):\n",
    "    (df, df_removed) = convert_json2df(file_dump[i].get('data'))\n",
    "    print(f\"-----------start file_dump{i}--------------\")\n",
    "    new_df = calculate_vacancies(df)\n",
    "    totaldf = pd.concat([totaldf, new_df])\n",
    "    totaldf.drop_duplicates(subset='ads_id', inplace=True)\n",
    "    processed_file += 1\n",
    "print(f\"finish the data total is {totaldf.shape[0]}\")\n",
    "totaldf.to_csv(\"/home/inlab4/Documents/Dan_datasets/AF_results/df2020-03-13.csv\")\n",
    "print(f\"total processed file {processed_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T14:33:10.660225Z",
     "start_time": "2020-03-13T14:33:09.987013Z"
    }
   },
   "outputs": [],
   "source": [
    "#column publication_date is datetime now\n",
    "%matplotlib qt\n",
    "totaldf['reference_year']= totaldf['publication_date'].apply(lambda x: x.strftime(\"%Y\"))\n",
    "totaldf['time'] = totaldf[['reference_year', 'reference_month']].apply(lambda x: datetime.datetime.strptime(\"%Y\") )\n",
    "groups = totaldf.groupby(['time'])['number_of_vacancies'].sum()\n",
    "groups.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T08:09:18.920929Z",
     "start_time": "2020-03-13T08:09:18.912917Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#read in two jsonfiles and compare the increasing number\n",
    "file_path1 = path+\"2020-02-11T08%3A45%3A16.txt\"\n",
    "file_path2 = path+\"2020-02-11.txt\"\n",
    "def find_increased(f1, f2):\n",
    "    # with the given loaded data jsonfile1 and jsonfile2 of two dates, comparing ads_id\n",
    "    #jsonfile2 loading date is late than jsonfile1\n",
    "    #return only the increaded\n",
    "    \n",
    "    with open(f1, 'r') as f:\n",
    "        data1 = json.load(f)\n",
    "    with open(f2, 'r') as f:\n",
    "        data2 = json.load(f)\n",
    "    df1 = convert_json2df(data1)\n",
    "    df2 = convert_json2df(data2)\n",
    "    online_df1 = df1.loc[df1.removed==False]\n",
    "    #use total data on date 2, since some may remove \n",
    "    merged_ads_outer = pd.merge(online_df1, df2, how='outer', on='ads_id')\n",
    "    merged_ads_inner = pd.merge(online_df1, df2, how='inner', on='ads_id')\n",
    "    increased = merged_ads_outer[merged_ads_outer.removed_y==False].shape[0]-merged_ads_inner.shape[0]\n",
    "    #how about increaed ads\n",
    "    return increased\n",
    "\n",
    "def get_increased_ads(f, date):\n",
    "    #by comparing giving a date, return ads that have bigger publication date\n",
    "    #f is the file downloaded on one day\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    df = convert_json2df(data)\n",
    "    result = df.loc[df.publication_date>=date]\n",
    "    return result\n",
    "\n",
    "#print(find_increased(file_path1, file_path2))\n",
    "#df = get_increased_ads(file_path2, datetime.date.fromisoformat('2020-02-10'))\n",
    "#df['publication_date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#show the number in bar or with map in the background???\n",
    "#first groupby municipality and then on nuts, finally draw in bars\n",
    "%matplotlib qt\n",
    "kommun_lan = pd.read_csv(\"/home/inlab4/Documents/Dan_datasets/kommun_lan.csv\", sep=\";\", dtype={\"Code\":str, \"nuts2\":str})\n",
    "#print(kommun_lan.head())\n",
    "def read_daily_data_address(fd):\n",
    "    #read in the data day by day with simple variabels + address\n",
    "    #fd is a dictionary structure created in the first cell, when the data were read in\n",
    "    total = read_one_day_dump(file_dump[0].get('data'))\n",
    "    #print(total.shape)\n",
    "    #we cannot see the column removed which is a simple variable, merge with total handle this\n",
    "    address = get_work_addresses(file_dump[0].get('data'))\n",
    "    #print(address.shape)\n",
    "    total = pd.merge(total, address, on='ads_id')\n",
    "    #print(total.describe())\n",
    "    return total\n",
    "\n",
    "totalad = pd.DataFrame()\n",
    "for i in range(len(file_dump)):\n",
    "    if i == 0:\n",
    "        totalad = read_daily_data_address(file_dump[i].get('data'))\n",
    "    else:\n",
    "        daydf = read_daily_data_address(file_dump[i].get('data'))\n",
    "        #add the new data\n",
    "        totalad = pd.concat([totalad, daydf])\n",
    "        totalad.drop_duplicates(subset=\"ads_id\", inplace=True)\n",
    "\n",
    "#check first null value in address_mulnicipality_code and \n",
    "print(totalad.info())\n",
    "print(totalad.address_coordinates.value_counts())\n",
    "#when all data are read in, use groupby and generate the region vacancies    \n",
    "#group_sum = totalad.groupby([\"address_municipality_code\", \"year\", \"month\"])['number_of_vacancies'].sum()\n",
    "#group_sum.sort_index(inplace=True)\n",
    "#annonserdf = pd.DataFrame()\n",
    "#annonserdf['municipality_code'] = group_sum.index.get_level_values(0)\n",
    "#annonserdf['date'] = [\"-\".join([y, m]) for (y,m) in zip(group_sum.index.get_level_values(1), group_sum.index.get_level_values(2))]\n",
    "#annonserdf['date'] = pd.to_datetime(annonserdf['date']) #this gives minutes seconds in the end of date\n",
    "#annonserdf['vacancies'] = group_sum.values\n",
    "#annonserdf = pd.merge(annonserdf, kommun_lan, left_on='municipality_code', right_on='Code')\n",
    "#print(annonserdf.info())\n",
    "#print(annonserdf)\n",
    "#nuts = ['SE11', 'SE12', 'SE21', 'SE22', 'SE23', 'SE31', 'SE32', 'SE33']\n",
    "#region_groups = annonserdf.groupby(['date', 'nuts2'])['vacancies'].sum().unstack('nuts2').fillna(0)\n",
    "\n",
    "#print(region_groups)\n",
    "#region_groups.plot(kind='bar', stacked=True) \n",
    "#plt.title(\"Total vacancies distributed in regions over Sweden\")\n",
    "#plt.xlabel(\"Time(month)\")\n",
    "#plt.ylabel(\"Vacancies\")\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import geopandas as gpd\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "countries = world[world['continent']=='Europe']\n",
    "sweden = countries[countries['name']=='Sweden']\n",
    "sweden.plot()\n",
    "print(sweden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "df1 = convert_json2df(file_dump[0].get('data'))\n",
    "df2 = convert_json2df(file_dump[1].get('data'))\n",
    "df3 = convert_json2df(file_dump[2].get('data'))\n",
    "df4 = convert_json2df(file_dump[3].get('data'))\n",
    "df5 = convert_json2df(file_dump[4].get('data'))\n",
    "df6 = convert_json2df(file_dump[6].get('data'))\n",
    "df7 = convert_json2df(file_dump[7].get('data'))\n",
    "df8 = convert_json2df(file_dump[8].get('data'))\n",
    "df9 = convert_json2df(file_dump[9].get('data'))\n",
    "\n",
    "dfs = [df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "total = dfs[0].loc[dfs[0].removed==False]\n",
    "\n",
    "for i in range(1, len(dfs)):\n",
    "    #add only open ads\n",
    "    #print(\"data frame is: dfs%d\"%i)\n",
    "    #\"remove\" of some ads may tunr to True from False\n",
    "  \n",
    "    subset = dfs[i].loc[dfs[i].removed==False]\n",
    "    #print(subset['removed'].describe())\n",
    "    total = pd.concat([total,subset], ignore_index=True)\n",
    "    #print(total.shape)\n",
    "\n",
    "#print(total.head())\n",
    "before_ads = total.shape[0]\n",
    "print(total['removed'].describe())\n",
    "total.drop_duplicates(subset=['ads_id'], inplace=True)\n",
    "after_ads = total.shape[0]\n",
    "#only online ads are aggregated, check if this variable is unique False\n",
    "print(total['removed'].describe())\n",
    "diff_ads = before_ads-after_ads\n",
    "print(\"Total loaded online ads: %d; Total after drop_duplicates: %d; Duplicated ads are: %d\"%(before_ads, after_ads, diff_ads))\n",
    "print(total['removed'].describe())\n",
    "#download the file into csv for DB\n",
    "#write down the date of the aggregation, how to do this more efficiently???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "\n",
    "\n",
    "total = parallelize_dataframe(total, add_dates2df)\n",
    "print(total.shape)\n",
    "print(total.head())\n",
    "print(\"execution time is %5.2f\"%(time.process_time()-start))\n",
    "print(total['salary_description'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7,
     12,
     34,
     44
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#show the description of the dataset\n",
    "online_df = total\n",
    "print(\"Total Data shape is (%d, %d)\"%online_df.shape)\n",
    "print(\"Description of number of vacancies\")\n",
    "print(online_df['number_of_vacancies'].describe())\n",
    "df_error = online_df.loc[online_df['number_of_vacancies'] == 0]\n",
    "print(df_error.shape)\n",
    "if df_error.shape[0]>0:\n",
    "    #print out text if some vacancies are registered in 0\n",
    "    print(\"Number of vacancies reported in 0: %d\"%df_error.shape[0])\n",
    "\n",
    "before = online_df['number_of_vacancies'].sum()\n",
    "if df_error.shape[0]>0: # if there are 0 in vacancies, handle it\n",
    "    online_df['number_of_vacancies'] = online_df['number_of_vacancies'].apply(lambda x: 1 if x==0 else (x))\n",
    "    print(\"after the controll of number of vacancies\")\n",
    "    total_vacancies = online_df['number_of_vacancies'].sum()\n",
    "    print(\"differences after check of number of vacancies: %d\"%(total_vacancies-before))\n",
    "\n",
    "#study the number of vacancies >10\n",
    "large_vacancies = online_df.loc[online_df['number_of_vacancies']>10]\n",
    "\n",
    "print(\"*******************************************\")\n",
    "#if 0 is identified in the dataset, do the following\n",
    "#online_df['number_of_vacancies'].fillna(0, inplace=True)   \n",
    "#online_df['number_of_vacancies'].describe()\n",
    "\n",
    "pdates = online_df['publication_date'].sort_values().tolist()\n",
    "print(len(pdates))\n",
    "print(\"first publication date is : %s\"%pdates[0])\n",
    "print(\"last publication date is: %s\"%pdates[-1])\n",
    "\n",
    "#is there any differences between last_pblucation_date and application_deadline??\n",
    "#create a list storing diff in days\n",
    "diff = []\n",
    "for row in online_df.itertuples():\n",
    "    diff_days = (row.last_publication_date - row.application_deadline).days\n",
    "    if diff_days>0:\n",
    "        diff.append(diff_days)\n",
    "if len(diff)>0:    \n",
    "    print(\"%d ads found differences between last_publication_date and application_deadline\"%len(diff))\n",
    "    print(diff)\n",
    "\n",
    "delta = []\n",
    "\n",
    "for row in online_df.itertuples():\n",
    "    delta.append((row.application_deadline-row.publication_date).days)\n",
    "    \n",
    "online_df['lasting_days'] = delta\n",
    "#print(online_df.head())\n",
    "#print(online_df.info())\n",
    "print(\"----------------Description of lasting days------------------\")\n",
    "print(online_df['lasting_days'].describe()) #can \n",
    "total_vacancies = online_df['number_of_vacancies'].sum()\n",
    "print(\"--------Total ads have vacancies: %d--------------\"%total_vacancies)\n",
    "print(\"Please write the maximum lasting days online of an adertisement allowed to be\")\n",
    "lasting_day = input(\"\")\n",
    "print(\"ads lasting more than %s days\"%lasting_day)\n",
    "current_ads = online_df.query('lasting_days<=%d'%int(lasting_day))\n",
    "print(current_ads.sort_values(by=['publication_date','lasting_days']))\n",
    "print(\"------ads lasting max %s days have total vacancies: %d------\"%(lasting_day, current_ads['number_of_vacancies'].sum()))\n",
    "print(\"------ads lasting max %s days have vacancies in %f of total vacancies\"%(lasting_day, current_ads['number_of_vacancies'].sum()/total_vacancies))\n",
    "print(\"------ads lasting max %s days have %f of total ads\"%(lasting_day, current_ads.shape[0]/online_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicates: first check ads that have lasting days shorter than 30\n",
    "ads_less30 = online_df.loc[online_df['lasting_days']<=30]\n",
    "ads_less30_total = ads_less30.shape[0]\n",
    "ads_less30_vacancies = ads_less30['number_of_vacancies'].sum()\n",
    "#one outlier has lasting days as 365\n",
    "#print(data_left.loc[(data_left.year=='2019') & (data_left.month=='05')])\n",
    "\n",
    "print(\"Ads lasting less than 30 days has Vacancies percentage {:.2f}\".format(ads_less30_vacancies/total_vacancies))\n",
    "print(ads_less30.head(10))\n",
    "\n",
    "#domain_list = ads_less30['webpage_url'].apply(lambda x: x.split('/')[2])\n",
    "#site_list = ads_less30['webpage_url'].apply(lambda x: x.split('/')[-1])\n",
    "#print(len(set(domain_list)))\n",
    "#print(site_list[10:15])\n",
    "#ads_less30['webpage_url'].values.tolist()[2:5]\n",
    "\n",
    "outlier_ads = online_df.loc[(online_df['lasting_days']<7) | (online_df['lasting_days']>61)]\n",
    "print(\"outlier ads shape: (%d, %d)\"%outlier_ads.shape)\n",
    "before = outlier_ads.shape[0]\n",
    "outlier_ads = outlier_ads.drop_duplicates(subset=['ads_id', 'webpage_url'])\n",
    "print(\"differences after the drop duplicates are %d \"%(outlier_ads.shape[0]-before))\n",
    "print(\"-------------Lasting days description-----------\")\n",
    "print(outlier_ads['lasting_days'].describe())\n",
    "\n",
    "#merge with employment type and examine how the ads are distributed over the employment type\n",
    "employer = get_employer_values(file_dump[1].get('data'))\n",
    "employment_type = get_commonstructure_type(file_dump[1].get('data'), 'employment_type')\n",
    "duration = get_commonstructure_type(file_dump[1].get('data'), 'duration')\n",
    "#heltid, deltid\n",
    "working_hours_type = get_commonstructure_type(file_dump[1].get('data'), 'working_hours_type')\n",
    "occupation_group = get_commonstructure_type(file_dump[1].get('data'), 'occupation_group')\n",
    "salary_type = get_commonstructure_type(file_dump[1].get('data'), 'salary_type')\n",
    "\n",
    "#merge with employer and study how outerliers associate with employer\n",
    "#outlier_ads = outlier_ads.merge(employer, on='ads_id', how=\"left\")\n",
    "#print(outlier_ads['employer_workplace'].value_counts())\n",
    "#print(outlier_ads.groupby(['employer_organization_number']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "#print(outlier_ads[outlier_ads.employer_organization_number=='5590903570'])\n",
    "#print(outlier_ads.head(10))\n",
    "#print(outlier_ads.groupby(['experience_required']).count())\n",
    "#print(outlier_ads.groupby(['month']).count())\n",
    "\n",
    "#study the outlier with the employment type\n",
    "#outlier_ads = outlier_ads.merge(employment_type, on='ads_id', how=\"left\")\n",
    "#print(outlier_ads.groupby(['employment_type_label']).count().sort_values(by=\"ads_id\", ascending=False).head(10))\n",
    "#print(outlier_ads['employment_type_label'].value_counts())\n",
    "#outlier_ads = outlier_ads.merge(duration, on='ads_id',how=\"left\")\n",
    "#print(outlier_ads['duration_legacy_ams_taxonomy_id'].value_counts())\n",
    "#print(outlier_ads.shape)\n",
    "#print(outlier_ads.groupby(['duration_label']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "\n",
    "#outlier_ads = outlier_ads.merge(working_hours_type, on='ads_id', how=\"left\")\n",
    "#print(outlier_ads.groupby(['working_hours_type_label']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "#print(outlier_ads['working_hours_type_legacy_ams_taxonomy_id'].value_counts())\n",
    "outlier_ads = outlier_ads.merge(occupation_group, on='ads_id', how='left')\n",
    "print(occupation_group['occupation_group_legacy_ams_taxonomy_id'].value_counts())\n",
    "print(outlier_ads.groupby(['occupation_group_label']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "\n",
    "#online_ads = pd.merge(online_df, salary_type, on='ads_id', how='left')\n",
    "#print(online_ads.groupby(['salary_type_label']).count().sort_values(by='ads_id', ascending=False).head(10))\n",
    "#print(online_ads['salary_description'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulization of totals vacancies\n",
    "print(\"Please write 'month' or 'day' for dawing total vacancies\")\n",
    "group_unit=input()\n",
    "\n",
    "if group_unit=='day':\n",
    "    #summary of daily vacancies\n",
    "    group_sum = online_ads.groupby(['year', 'month', 'day'])['number_of_vacancies'].sum()\n",
    "    #print(group_sum)\n",
    "    #convert a panda group multiindex into a list\n",
    "    annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "    annonserdf['date'] = annonserdf['date'].apply(lambda x: datetime.date.fromisoformat('-'.join([x[0], x[1], x[2]])))\n",
    "    annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "    print(annonserdf.head())\n",
    "    print(annonserdf.describe())\n",
    "    print(\"---check if some days have no vacancies---\")\n",
    "    print(annonserdf.loc[annonserdf['sum']==0.0])\n",
    "    ax = annonserdf.plot(x='date', y='sum', marker='*')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"/home/inlab4/Documents/AF/graphs/vacancies_daily.jpg\")\n",
    "elif group_unit=='month':\n",
    "    group_sum = online_ads.groupby(['year', 'month'])['number_of_vacancies'].sum()\n",
    "    #print(group_sum)\n",
    "    #convert a panda group multiindex into a list\n",
    "    annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "    annonserdf['date']=annonserdf['date'].apply(lambda x: datetime.datetime.strptime(\"-\".join([x[0], x[1]]), \"%Y-%m\"))\n",
    "    annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "    print(annonserdf.head())\n",
    "    print(annonserdf.describe())\n",
    "    print(\"---check if some days have no vacancies---\")\n",
    "    print(annonserdf.loc[annonserdf['sum']==0.0])\n",
    "    ax = annonserdf.plot(x='date', y='sum', marker='*')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"/home/inlab4/Documents/AF/graphs/vacancies_monthly.jpg\")\n",
    "else:\n",
    "    print(\"Please give a group unit in 'month' or 'day'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_text = []\n",
    "ansok_url = []\n",
    "import re\n",
    "urlpattern = re.compile('\\[http\\S\\]')\n",
    "for item in data:\n",
    "    if item.get('description') != None:\n",
    "        if item['description'].get('text') != None:\n",
    "            description_text.append(item['description']['text'])\n",
    "            \n",
    "            url = urlpattern.findall(item['description']['text'])\n",
    "                break\n",
    "            if url:\n",
    "                ansok_url.append(url[0])\n",
    "            else:\n",
    "                ansok_url.append('NaN')\n",
    "    else:\n",
    "        description_text.append('NaN')\n",
    "        ansok_url.append('NaN')\n",
    "\n",
    "print(len(description_text))\n",
    "print(description_text[100])\n",
    "print(ansok_url)\n",
    "s = \"39053\n",
    "KVALIFIKATIONER \n",
    "\n",
    "• Sjuksköterskeexamen med minst 1års yrkeserfarenhet som skolsköterska.\n",
    " • VUB inom område\n",
    " • Mycket goda kunskaper i Svenska, både i tal och skrift.\n",
    " • Meriterandemed erfarenhet avvaccination \n",
    "\n",
    "ANSÖKAN \n",
    "\n",
    "Du är varmt välkommen att skicka in din ansökan till oss. Ansök genom att klicka på knappen ”Ansök här”. Om du som sökande har frågor om den utannonserade tjänsten var vänlig och kontakta kontaktpersonen för denna annons. På vårhemsida [https://www.dedicare.se/yrkesroll/sjukskoterska/]presenterar vi alla våra lediga uppdrag. Urval och intervjuer sker löpande, vi tar tacksamt emot din ansökan snarast. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_id = []\n",
    "label= []\n",
    "legacy_ams_taxonomy_id = []\n",
    "\n",
    "for item in data1[0:3]:\n",
    "    employment_type = item.get('employment_type')\n",
    "    if isinstance(employment_type, dict):\n",
    "        concept_id.append(employment_type.get('concept_id'))\n",
    "        label.append(employment_type.get('label'))\n",
    "        legacy_ams_taxonomy_id.append(employment_type.get('legacy_ams_taxonomy_id'))\n",
    "\n",
    "    else:\n",
    "        concept_id.append(np.nan)\n",
    "        label.append(np.nan)\n",
    "        legacy_ams_taxonomy_id.append(np.nan)\n",
    "        \n",
    "print(concept_id)\n",
    "print(label)\n",
    "print(legacy_ams_taxonomy_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sum = data_left.groupby(['year', 'month'])['number_of_vacancies'].sum()\n",
    "#print(group_sum)\n",
    "#convert a panda group multiindex into a list\n",
    "annonserdf= pd.DataFrame({'date':group_sum.index.tolist(), 'sum': group_sum.values})\n",
    "annonserdf['date']=annonserdf['date'].apply(lambda x: datetime.datetime.strptime(\"-\".join([x[0], x[1]]), \"%Y-%m\"))\n",
    "annonserdf = annonserdf.sort_values(['date'], ascending=True)\n",
    "print(annonserdf.head())\n",
    "print(annonserdf.describe())\n",
    "print(\"---check if some days have no vacancies---\")\n",
    "print(annonserdf.loc[annonserdf['sum']==0.0])\n",
    "ax = annonserdf.plot(x='date', y='sum', marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:20:40.197397Z",
     "start_time": "2020-03-12T15:20:40.187849Z"
    }
   },
   "outputs": [],
   "source": [
    "date1 = 3\n",
    "date2 = 7\n",
    "for i in range(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T11:19:12.746731Z",
     "start_time": "2020-03-12T11:19:12.473745Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "html = requests.get(\"http://www.coopans.com\", timeout=5, headers={})\n",
    "if html.status_code == 200:\n",
    "    f = open('/home/inlab/CISNLP/Dan/coopans.html', 'w+')\n",
    "    f.write(page_html.text)\n",
    "    f.close()\n",
    "    print(\"created file\")\n",
    "else:\n",
    "    print(\"error\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T14:52:31.142947Z",
     "start_time": "2020-03-13T14:52:00.373Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({'Year': ['2014', '2015'], 'quarter': ['q1', 'q2']})\n",
    "df_test['period'] = df['Year'], 'quarter']].apply(lambda x: ''.join(x), axis=0)\n",
    "print(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T14:48:25.615439Z",
     "start_time": "2020-03-13T14:48:25.604579Z"
    }
   },
   "outputs": [],
   "source": [
    "date = datetime.datetime.strptime('2020-03-31T23:59:59', '%Y-%m-%dT%H:%M:%S')\n",
    "print(date)\n",
    "print(type(date))\n",
    "print(date.strftime(\"%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T12:04:37.046699Z",
     "start_time": "2020-03-19T12:04:37.035581Z"
    }
   },
   "outputs": [],
   "source": [
    "s1 = \"hi, i am david\"\n",
    "\n",
    "print(s1[4: len(s1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "434px",
    "left": "1448px",
    "right": "20px",
    "top": "119px",
    "width": "343px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
